This file is a merged representation of the entire codebase, combined into a single document by Repomix.

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded

Additional Info:
----------------

================================================================
Directory Structure
================================================================
API/
  AI.py
helpers/
  analogy_helper.py
  analogy_stream_helper.py
  async_tasks.py
  celery_app.py
  daily_newsletter_helper.py
  daily_newsletter_task.py
  email_helper.py
  grc_helper.py
  grc_stream_helper.py
  log_generator.py
  log_helper.py
  pbq_ai_helper.py
  scenario_helper.py
  status_helper.py
  xploitcraft_helper.py
models/
  log_history.py
  log_models.py
  newsletter_content.py
  test.py
  user_subscription.py
mongodb/
  database.py
routes/
  admin_newsletter_routes.py
  analogy_routes.py
  celery_routes.py
  daily_brief_routes.py
  grc_routes.py
  log_routes.py
  pbq_routes.py
  scenario_routes.py
  status_routes.py
  subscribe_routes.py
  test_routes.py
  unsubscribe_routes.py
  xploit_routes.py
app.py
Dockerfile.backend
requirements.txt
update_newsletter.py

================================================================
Files
================================================================

================
File: API/AI.py
================
import os
import logging
from openai import OpenAI
from dotenv import load_dotenv


load_dotenv()


logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


def load_api_key() -> str:
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        logger.error("OpenAI API key is missing. Please ensure it's set in the environment variables.")
        raise ValueError("OpenAI API key is required but not found.")
    return api_key


api_key = load_api_key()
client = OpenAI(api_key=api_key)

================
File: helpers/analogy_helper.py
================
import os
import logging
from API.AI import client


logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

def generate_single_analogy(concept, category):
    """
    Generate a single analogy for the given concept and category.
    """
    prompt = (
        f"Generate an analogy for the concept '{concept}' using the context of '{category}'. "
        "Make it easy to understand but informative and in a teaching style, concise but in depth, and entertaining,  with one key info at the end to make sure the info is remembered.Do not explicilty say that you will create the analogy just output the analogy/explantion only e.g to not show: Sure! Let's dive into the fascinating world of cybersecurity using an analogy that you might find both informative and entertaining or any other variants"
    )

    try:
        response = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="gpt-4o",
            max_tokens=750,
            temperature=0.7,
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        logger.error(f"Error generating single analogy: {e}")
        return "An error occurred while generating the analogy."

def generate_comparison_analogy(concept1, concept2, category):
    """
    Generate a comparison analogy between two concepts and a category.
    """
    prompt = (
        f"Compare '{concept1}' and '{concept2}' using an analogy in the context of '{category}'. "
        "Explain how they are similar and different or how they might work in conjunction with each other, in a teaching style, informative, concise but in depth, and entertaining,  with one key info at the end to make sure the info is rememebered. Do not explicilty say that you will create the analogy just output the analogy/explantion only e.g to not show: Sure! Let's dive into the fascinating world of cybersecurity using an analogy that you might find both informative and entertaining or any other variants"
    )

    try:
        response = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="gpt-4o",
            max_tokens=1000,
            temperature=0.7,
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        logger.error(f"Error generating comparison analogy: {e}")
        return "An error occurred while generating the analogy."

def generate_triple_comparison_analogy(concept1, concept2, concept3, category):
    """
    Generate a comparison analogy among three concepts and a category.
    """
    prompt = (
        f"Compare '{concept1}', '{concept2}', and '{concept3}' using an analogy in the context of '{category}'. "
        "Explain how they are similar and different or how they might work in conjuction with each other, in a teaching style, informative, concise but in depth, and entertaining, with one key info at the end to make sure the info is rememebered.Do not explicilty say that you will create the analogy just output the analogy/explantion only e.g to not show: Sure! Let's dive into the fascinating world of cybersecurity using an analogy that you might find both informative and entertaining or any other variants"
    )

    try:
        response = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="gpt-4o",
            max_tokens=1200,
            temperature=0.7,
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        logger.error(f"Error generating triple comparison analogy: {e}")
        return "An error occurred while generating the analogy."

================
File: helpers/analogy_stream_helper.py
================
import logging
from API.AI import client

logger = logging.getLogger(__name__)

def generate_analogy_stream(analogy_type, concept1, concept2, concept3, category):
    """
    Streams the analogy text in partial chunks (NOT forcibly splitting by word),
    so the front-end can handle how to display it (word-by-word or otherwise).
    """


    if analogy_type == "comparison" and concept2:
        concept_part = f"Compare {concept1} and {concept2}"
    elif analogy_type == "triple" and concept2 and concept3:
        concept_part = f"Compare {concept1}, {concept2}, and {concept3}"
    else:

        concept_part = f"Generate an analogy for the concept {concept1}"


    prompt = (
        f"{concept_part} using the context of {category}. "
        "Make it easy to understand yet informative, in a teaching style, concise but in depth, and entertaining, "
        "with a final memory hook. Do NOT say 'Sure, let's dive in'; just provide the analogy only."
    )

    try:
        response = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="gpt-4o",
            max_tokens=1200,
            temperature=0.7,
            stream=True
        )

        def generator():
            try:
                for chunk in response:
                    delta = chunk.choices[0].delta
                    if delta:
                        content = getattr(delta, "content", None)
                        if content:

                            yield content
            except Exception as e:
                logger.error(f"Error streaming analogy: {e}")

                yield ""

        return generator()

    except Exception as e:
        logger.error(f"Error generating analogy stream: {e}")

        def err_gen():
            yield ""
        return err_gen()

================
File: helpers/async_tasks.py
================
# helpers/async_tasks.py

import logging
from helpers.celery_app import app

from helpers.analogy_helper import (
    generate_single_analogy as _generate_single_analogy,
    generate_comparison_analogy as _generate_comparison_analogy,
    generate_triple_comparison_analogy as _generate_triple_comparison_analogy
)

from helpers.scenario_helper import (
    generate_scenario as _generate_scenario,
    break_down_scenario as _break_down_scenario,
    generate_interactive_questions as _generate_interactive_questions  
)

from helpers.xploitcraft_helper import Xploits as _Xploits

from helpers.grc_helper import generate_grc_question as _generate_grc_question

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)


@app.task(bind=True, max_retries=3, default_retry_delay=10)
def generate_single_analogy_task(self, concept, category):
    try:
        return _generate_single_analogy(concept, category)
    except Exception as e:
        logger.error(f"Celery generate_single_analogy_task error: {e}")
        self.retry(exc=e)


@app.task(bind=True, max_retries=3, default_retry_delay=10)
def generate_comparison_analogy_task(self, concept1, concept2, category):
    try:
        return _generate_comparison_analogy(concept1, concept2, category)
    except Exception as e:
        logger.error(f"Celery generate_comparison_analogy_task error: {e}")
        self.retry(exc=e)


@app.task(bind=True, max_retries=3, default_retry_delay=10)
def generate_triple_comparison_analogy_task(self, concept1, concept2, concept3, category):
    try:
        return _generate_triple_comparison_analogy(concept1, concept2, concept3, category)
    except Exception as e:
        logger.error(f"Celery generate_triple_comparison_analogy_task error: {e}")
        self.retry(exc=e)


@app.task(bind=True, max_retries=3, default_retry_delay=10)
def generate_scenario_task(self, industry, attack_type, skill_level, threat_intensity):
    """
    If _generate_scenario now returns a generator (streaming), we can consume it fully here,
    returning a single string to Celery.
    """
    try:
        scenario_gen = _generate_scenario(industry, attack_type, skill_level, threat_intensity)
        scenario_text = "".join(scenario_gen)
        return scenario_text
    except Exception as e:
        logger.error(f"Celery generate_scenario_task error: {e}")
        self.retry(exc=e)


@app.task(bind=True, max_retries=3, default_retry_delay=10)
def break_down_scenario_task(self, scenario_text):
    try:
        return _break_down_scenario(scenario_text)
    except Exception as e:
        logger.error(f"Celery break_down_scenario_task error: {e}")
        self.retry(exc=e)


@app.task(bind=True, max_retries=3, default_retry_delay=10)
def generate_interactive_questions_task(self, scenario_text):
    """
    If _generate_interactive_questions now yields streaming tokens, 
    we just gather them into one final string here.
    """
    try:
        questions_gen = _generate_interactive_questions(scenario_text)
        questions_text = "".join(questions_gen)
        return questions_text
    except Exception as e:
        logger.error(f"Celery generate_interactive_questions_task error: {e}")
        self.retry(exc=e)


_xploit = _Xploits()

@app.task(bind=True, max_retries=3, default_retry_delay=10)
def generate_exploit_payload_task(self, vulnerability, evasion_technique):
    try:
        return _xploit.generate_exploit_payload(vulnerability, evasion_technique)
    except Exception as e:
        logger.error(f"Celery generate_exploit_payload_task error: {e}")
        self.retry(exc=e)


@app.task(bind=True, max_retries=3, default_retry_delay=10)
def generate_grc_question_task(self, category, difficulty):
    try:
        return _generate_grc_question(category, difficulty)
    except Exception as e:
        logger.error(f"Celery generate_grc_question_task error: {e}")
        self.retry(exc=e)

================
File: helpers/celery_app.py
================
# helpers/celery_app.py

import os
import logging
from celery import Celery
from celery.schedules import crontab
from dotenv import load_dotenv


load_dotenv()
SENDGRID_API_KEY = os.getenv("SENDGRID_API_KEY")


logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

logger.debug(f"SendGrid API Key: {SENDGRID_API_KEY}")

CELERY_BROKER_URL = os.getenv("CELERY_BROKER_URL", "redis://redis:6379/0")
CELERY_RESULT_BACKEND = os.getenv("CELERY_RESULT_BACKEND", "redis://redis:6379/0")

app = Celery(
    'tasks',
    broker=CELERY_BROKER_URL,
    backend=CELERY_RESULT_BACKEND,
    broker_connection_retry_on_startup=True,
    include=[
        'helpers.async_tasks',
        'helpers.daily_newsletter_task'  
    ]
)

app.conf.update({
    'worker_prefetch_multiplier': 1,
    'task_acks_late': True,
    'worker_concurrency': 8,
    'timezone': 'America/New_York',  
    'enable_utc': True,  
})


app.conf.beat_schedule = {
    'send-daily-newsletter-midnight': {
        'task': 'helpers.daily_newsletter_task.send_daily_newsletter',
        'schedule': crontab(hour=0, minute=0),  
    },
}

app.autodiscover_tasks(['helpers'])

logger.info("Celery app initialized with broker %s and backend %s", CELERY_BROKER_URL, CELERY_RESULT_BACKEND)

================
File: helpers/daily_newsletter_helper.py
================
# helpers/daily_newsletter_helper.py

import logging
from models.newsletter_content import (
    get_current_newsletter_db,
    set_current_newsletter_db
)

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

def set_newsletter_content(content):
    set_current_newsletter_db(content)

def get_newsletter_content():
    doc = get_current_newsletter_db()
    return doc.get("content", "")

================
File: helpers/daily_newsletter_task.py
================
# helpers/daily_newsletter_task.py

from helpers.celery_app import app
from helpers.email_helper import send_email
from models.user_subscription import get_all_subscribers
from helpers.daily_newsletter_helper import get_newsletter_content
import logging

logger = logging.getLogger(__name__)

@app.task(name="helpers.daily_newsletter_task.send_daily_newsletter")
def send_daily_newsletter():
    """
    Celery task: send daily newsletter to all subscribers.
    """
    try:
        content = get_newsletter_content()
        if not content:
            logger.warning("No newsletter content found.")
            return

        subscribers = get_all_subscribers()
        if not subscribers:
            logger.warning("No subscribers found.")
            return

        for subscriber in subscribers:
            email = subscriber.get("email")
            if email:
                success = send_email(
                    to_email=email,
                    subject="Daily Cyber Brief",
                    content=content
                )
                if success:
                    logger.info(f"Newsletter sent to {email}")
                else:
                    logger.warning(f"Failed to send newsletter to {email}")

        logger.info("Daily newsletter task completed.")
    except Exception as e:
        logger.error(f"Error sending daily newsletter: {e}")
        raise e

================
File: helpers/email_helper.py
================
import os
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from dotenv import load_dotenv
import sendgrid
import os
from sendgrid.helpers.mail import Mail, Email, To, Content
from sendgrid import SendGridAPIClient



load_dotenv()


def send_email(to_email, subject, content):
    sg_api_key = os.getenv("SENDGRID_API_KEY")

    if not sg_api_key:
        print("SendGrid API key not found. Please ensure it is set in your environment variables.")
        return False

    
    message = Mail(
        from_email='dailycyberbrief@proxyauthrequired.com',  
        to_emails=to_email,
        subject=subject,
        html_content=content
    )

    try:
        sg = SendGridAPIClient(sg_api_key)
        response = sg.send(message)
        print(f"Email sent: Status Code: {response.status_code}")
        print(f"Response Body: {response.body}")
        print(f"Response Headers: {response.headers}")
        return True
    except Exception as e:
        print(f"Error sending email: {str(e)}")
        return False

================
File: helpers/grc_helper.py
================
import os
import json
import logging
import re  
from API.AI import client

logger = logging.getLogger(__name__)

def generate_grc_question(category, difficulty):
    """
    Generates a GRC-related multiple-choice question in JSON format.
    The model returns a JSON object with keys:
      question (string)
      options (array of 4 strings)
      correct_answer_index (int)
      explanations (dict of strings for "0","1","2","3")
      exam_tip (string)
    """

    prompt = f""" 
You are an expert in concepts found in certifications like CISSP, CompTIA Advanced Security Practitioner (CASP+), CISM, CRISC, and others. Your role is to generate 
challenging and diverse test questions using advanced mult-layered reasoning, related to governance, risk management, risk thresholds, types of risk, Audit, Management, Policy, Cyber Security Ethics, Threat Assessment, 
Leadership, Business Continuity, compliance, regulations, incident resposne, Incident Response and more. focusing on preparing for exams like CISSP/ISC2 and CompTIA certifications. Ensure the questions cover a wide range of scenarios,
principles, and concepts, with multiple-choice answers that are nuanced and complex and specific, avoiding repetitive patterns or overly simplified examples.

CONTEXT: The user has selected:
- Category: {category} (e.g., 'Regulation', 'Risk Management', 'Compliance', 'Audit', 'Governance', 'Management', 'Policy', 'Ethics', 'Threat Assessment', 'Leadership', 'Business Continuity', 'Incident Response', 'Random')
- Difficulty: {difficulty} (e.g., 'Easy', 'Medium', 'Hard')

REQUIREMENTS
1. Four options (0, 1, 2, 3) total, one correct answer. The incorrect options should be very plausible but not correct, requiring the test-taker to carefully differentiate.

2. Explanations:
   - For the correct answer: Provide multiple sentences detailing exactly why it’s correct, clearly tying it back to the question’s scenario or concept. Show how it fulfills the requirements asked in the question as well as why the other answer choices are incorrect/not the correct answer..
   - For each incorrect answer: Provide multiple sentences detailing why it is NOT correct aswell as why the other incorrect answer choices are incorrect, and why then tell the user what the correct answer is and why it is correct using advanced multi-layered reasoning. 
     Do not just say it’s incorrect; fully explain why it falls short. 
     Highlight conceptual differences, limitations, or focus areas that differ from the question’s criteria.
   - Regardless of user choice, the generated output must contain full explanations for all answer choices provided. The explanations are produced in advance as part of the JSON object. Each explanation should be at least 3 sentences, rich in detail and conceptual clarity using advanced multi-layered reasoning.

3. Include an "exam_tip" field that provides a short, memorable takeaway or mnemonic to help differentiate the correct concept from the others. The exam tip should help the user recall why the correct answer stands out using advanced multi-layered reasoning.

4. Return ONLY a JSON object with the fields:
   "question", "options", "correct_answer_index", "explanations", and "exam_tip"
   No extra text, no Markdown, no commentary outside the JSON.

5. For each explanation (correct and incorrect):
   - At minimum of 3 sentences for the correct answer.
   - if the user gets the answer correct provide minium 3 senetence answer as to why it is correct, but also why the other answer choices listed are not the correct answer using advanced multi-layered reasoning.
   - Substantial detail.
   - Clearly articulate conceptual reasons, not just factual statements using advanced multi-layered reasoning.

EXAMPLE FORMAT (this is not real content, just structure, make sure to use all topics not just the topic provided in this example):
{{
  "question": "The question",
  "options": ["Option 0","Option 1","Option 2","Option 3"],
  "correct_answer_index": 2,
  "explanations": {{
    "0": "Explain thoroughly why option 0 fails. Mention its scope, focus areas, and why that doesn't meet the question criteria and then explain what the correct answer is and why it is correct aswell as why the other answer choices are incorrect using advanced multi-layered reasoning.",
    "1": "Explain thoroughly why option 1 fails. Mention its scope, focus areas, and why that doesn't meet the question criteria and then explain what the correct answer is and why it is correct aswell as why the other answer choices are incorrect using advanced multi-layered reasoning.",
    "2": "Explain thoroughly why option 2 is correct, linking its characteristics to the question scenario and why the other answer choices are incorrect using advanced multi-layered reasoning",
    "3": "Explain thoroughly why option 3 fails. Mention its scope, focus areas, and why that doesn't meet the question criteria and then explain what the correct answer is and why it is correct aswell as why the other answer choices are incorrect using advanced multi-layered reasoning."
  }},
  "exam_tip": "A short, memorable hint or mnemonic that differentiates the correct approach from others using advanced multi-layered reasoning."
}}

Now generate the JSON object following these instructions.
"""



    try:
        response = client.chat.completions.create(
            model="gpt-4o",  
            messages=[{"role": "user", "content": prompt}],
            max_tokens=900,
            temperature=0.6,
        )

        content = response.choices[0].message.content.strip()

      
        content = re.sub(r'^```.*\n', '', content)
        content = re.sub(r'\n```$', '', content)

        try:
            generated_question = json.loads(content)
        except json.JSONDecodeError as e:
            logger.error("JSON parsing error in generate_grc_question: %s", e)
            logger.error("Model returned: %s", content)
            raise ValueError("Model did not return valid JSON.") from e

        logger.info("Generated GRC question successfully.")
        return generated_question

    except Exception as e:
        logger.error(f"Error generating GRC question: {str(e)}")
        raise

================
File: helpers/grc_stream_helper.py
================
import logging
import json
from API.AI import client

logger = logging.getLogger(__name__)

def generate_grc_questions_stream(category, difficulty):
    """
    Streams EXACTLY THREE advanced GRC questions in a JSON array, but chunk-by-chunk
    rather than word-by-word. This means we yield partial content from GPT as it arrives
    without splitting on spaces. The front end can display partial JSON in real time.
    """

    prompt = f"""
You are an expert in concepts found in certifications like CISSP, CompTIA Advanced Security Practitioner (CASP+), CISM, CRISC, and others. 
Your role is to generate challenging and diverse test questions related to governance, risk management, risk thresholds, types of risk, 
Audit, Management, Policy, Cyber Security Ethics, Threat Assessment, Leadership, Business Continuity, compliance, regulations, 
incident response, and more, focusing on preparing for exams like CISSP and CompTIA certifications. Ensure the questions cover a wide 
range of scenarios, principles, and concepts, with multiple-choice answers that are nuanced, complex, and specific, avoiding repetitive 
patterns or overly simplified examples.

CONTEXT: The user has selected:
- Category: {category}
- Difficulty: {difficulty}

REQUIREMENTS:
1. Generate EXACTLY 3 questions in one JSON array. Each question has:
   - "question": string,
   - "options": array of exactly 4 strings (indexes 0,1,2,3),
   - "correct_answer_index": integer (0,1,2,3),
   - "explanations": object with keys "0","1","2","3" (multi-sentence detail),
   - "exam_tip": short mnemonic/hint.

2. The correct answer's explanation has at least 3 sentences describing precisely why it is correct, 
   and also clarifies why the others are incorrect.

3. Each incorrect answer's explanation has multiple sentences explaining why it is wrong, 
   plus clarifies what the correct choice is and why the other answer choices are also incorrect or less suitable.

4. Provide an "exam_tip" as a short, memorable mnemonic or hint to help the test-taker recall the correct concept.

5. Return ONLY the JSON array with exactly 3 objects. No extra text, disclaimers, or preludes.

6. Each explanation must be at least 3 sentences, offering substantial detail and conceptual clarity.

Now generate the JSON object following these instructions. 
Remember: 3 items in the array, each question shaped as above, nothing else.
"""

    try:
        # Make the streaming request to GPT
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=3000,  # Adjust as needed
            temperature=0.7,  # Adjust as needed
            stream=True
        )

        def generator():
            try:
                for chunk in response:
                    delta = chunk.choices[0].delta
                    if delta:
                        content = getattr(delta, "content", None)
                        if content:
                            # **Chunk-based** streaming (no splitting on spaces):
                            yield content
            except Exception as e:
                logger.error(f"Error streaming GRC questions: {e}")
                yield ""

        return generator()

    except Exception as e:
        logger.error(f"Error generating GRC questions (stream): {e}")

        def err_gen():
            yield ""
        return err_gen()

================
File: helpers/log_generator.py
================
import random
import json
from faker import Faker
from datetime import datetime
from typing import List
from models.log_models import (
    SecurityLog, FirewallLog, VulnerabilityLog, IntrusionLog, AccessControlLog,
    EventLog, SystemEvent, ApplicationEvent, AuthenticationEvent, NetworkEvent,
    ErrorLog, DatabaseErrorLog, FileSystemErrorLog, NetworkErrorLog, ApplicationErrorLog,
    DebugLog, QueryDebugLog, ApiDebugLog, ConfigDebugLog, ProcessDebugLog,
    InfoLog, SystemInfoLog, UserActivityLog, DeploymentLog, ServiceStatusLog,
    Log
)

fake = Faker()

def generate_firewall_log() -> FirewallLog:
    severity = random.choice(["critical", "high", "medium", "low"])
    geolocation = {
        "source_latitude": f"{fake.latitude()}",
        "source_longitude": f"{fake.longitude()}",
        "destination_latitude": f"{fake.latitude()}",
        "destination_longitude": f"{fake.longitude()}"
    }
    device_info = {
        "source_device": fake.user_agent(),
        "destination_device": fake.user_agent()
    }
    # Added a few more rule choices
    rule_triggered = random.choice([
        "Rule 101 - Suspicious Activity", "Rule 202 - High Traffic Volume", "Rule 303 - Unauthorized Scanning",
        "Rule 404 - Known Malicious IP", "Rule 505 - Protocol Anomaly"
    ])
    attack_vector_choices = ["Brute-force", "Phishing", "SQL Injection", "Cross-Site Scripting", "RFI", "LFI"]
    return FirewallLog(
        type="security",
        source="Firewall",
        event="Traffic Control",
        message=f"Blocked {random.choice(['incoming', 'outgoing'])} traffic from {fake.ipv4()} to {fake.ipv4()} due to rule violation.",
        severity=severity,
        source_ip=fake.ipv4(),
        destination_ip=fake.ipv4(),
        action=random.choice(["Allowed", "Blocked"]),
        protocol=random.choice(["TCP", "UDP", "ICMP"]),
        port=random.randint(1, 65535),
        rule_triggered=rule_triggered,
        traffic_volume=f"{random.randint(100, 10000)}MB",
        session_id=fake.uuid4() if random.choice([True, False]) else None,
        destination_country=fake.country(),
        source_country=fake.country(),
        ip_address=fake.ipv4(),
        username=fake.user_name(),
        attack_vector=random.choice(attack_vector_choices),
        status=random.choice(["Successful", "Failed"]),
        geolocation=geolocation,
        device_info=device_info
    )

def generate_vulnerability_log() -> VulnerabilityLog:
    severity_level = random.choice(["Low", "Medium", "High", "Critical"])
    geolocation = {
        "scanner_location": fake.city(),
        "target_location": fake.city()
    }
    device_info = {
        "scanner_device": fake.user_agent(),
        "target_device": fake.user_agent()
    }
    # More vulnerability names and tools
    vuln_name_choices = [
        "SQL injection flaw", "XSS vulnerability", "Privilege escalation bug", "Directory traversal issue",
        "Open redirect", "Command injection hole", "Insecure deserialization"
    ]
    tool_choices = ["Burp Suite", "Nessus", "Qualys", "OpenVAS", "Acunetix"]
    return VulnerabilityLog(
        type="security",
        source="Vulnerability Scanner",
        event="Scan Result",
        message=f"{random.choice(vuln_name_choices)} found on {fake.hostname()} affecting {random.choice(['database', 'API endpoint', 'user interface', 'authentication flow'])}.",
        severity=severity_level.lower(),
        tool=random.choice(tool_choices),
        vulnerability_name=fake.catch_phrase(),
        cve_id=f"CVE-{random.randint(1999, 2024)}-{random.randint(1000, 9999)}",
        severity_level=severity_level,
        remediation_steps=[
            f"Apply patch {fake.numerify('##')}",
            f"Restrict access to {fake.domain_word()}",
            f"Update configuration files for {fake.bs()}",
            "Implement input validation", "Enable WAF rules"
        ],
        discovery_date=fake.date_time_between(start_date='-60d', end_date='now'),
        patch_available=random.choice([True, False]),
        geolocation=geolocation,
        device_info=device_info,
        ip_address=fake.ipv4(),
        username=fake.user_name(),
        attack_vector=random.choice(["Brute-force", "Phishing", "SQL Injection", "XSS", "LFI", "RCE"]),
        status=random.choice(["Successful", "Failed"]),
        affected_components=[fake.bs().capitalize() for _ in range(random.randint(1, 4))]
    )

def generate_intrusion_log() -> IntrusionLog:
    risk_level = random.choice(["Low", "Medium", "High", "Critical"])
    geolocation = {
        "source_latitude": f"{fake.latitude()}",
        "source_longitude": f"{fake.longitude()}",
        "target_latitude": f"{fake.latitude()}",
        "target_longitude": f"{fake.longitude()}"
    }
    device_info = {
        "source_device": fake.user_agent(),
        "target_device": fake.user_agent()
    }
    attempted_payloads = [fake.word() for _ in range(random.randint(1, 3))]
    response_actions = [random.choice(["Alert Sent", "Connection Terminated", "IP Banned", "Session Terminated"]) for _ in range(random.randint(1,2))]

    return IntrusionLog(
        type="security",
        source="IDS",
        event="Unauthorized Access Attempt",
        message=f"Detected unauthorized access attempt from {fake.ipv4()} targeting {fake.hostname()}.",
        severity=random.choice(["critical", "high", "medium", "low"]),
        source_ip=fake.ipv4(),
        destination_ip=fake.ipv4(),
        username=fake.user_name(),
        attack_vector=random.choice(["Phishing", "Brute Force", "SQL Injection", "RCE", "LFI", "XSS"]),
        status=random.choice(["Blocked", "Failed", "Successful"]),
        detection_system=random.choice(["Snort", "Suricata", "OSSEC", "Zeek"]),
        intrusion_method=random.choice(["Exploit", "Privilege Escalation", "RDP Bruteforce", "SMB Lateral Movement"]),
        risk_level=risk_level,
        attempted_payloads=attempted_payloads,
        response_actions=response_actions,
        breach_confirmed=random.choice([True, False]),
        compromised_data=[fake.word() for _ in range(random.randint(1, 5))] if random.choice([True, False]) else None,
        geolocation=geolocation,
        device_info=device_info
    )

def generate_access_control_log() -> AccessControlLog:
    reason_choices = ["Unauthorized user", "Incorrect credentials", "Policy violation", "Resource locked", "MFA required"]
    return AccessControlLog(
        type="security",
        source="Access Control",
        event="Access Attempt",
        message=f"Access {'granted' if random.choice([True, False]) else 'denied'} to {fake.user_name()} for {fake.file_path()}.",
        severity=random.choice(["critical", "high", "medium", "low"]),
        ip_address=fake.ipv4(),
        username=fake.user_name(),
        attack_vector=random.choice(["Brute-force", "Privilege Escalation"]),
        status=random.choice(["Failed", "Successful"]),
        access_type=random.choice(["Granted", "Denied"]),
        resource=fake.file_path(),
        access_reason=random.choice(reason_choices),
        access_time=fake.date_time_between(start_date='-30d', end_date='now'),
        access_location=fake.city(),
        authentication_method=random.choice(["Password", "MFA", "Biometric", "Certificate"]),
        user_agent=fake.user_agent(),
        session_duration=f"{random.randint(5, 480)} minutes"
    )

def generate_security_logs(count: int) -> List[SecurityLog]:
    logs = []
    for _ in range(count):
        logs.append(generate_firewall_log())
        logs.append(generate_vulnerability_log())
        logs.append(generate_intrusion_log())
        logs.append(generate_access_control_log())
    return logs

def generate_system_event() -> SystemEvent:
    resource_usage = {
        "CPU": f"{random.randint(10, 90)}%",
        "RAM": f"{random.randint(20, 80)}%",
        "Disk": f"{random.randint(30, 95)}%"
    }
    network_interfaces = [f"eth{random.randint(0,3)}: {fake.ipv4()} - {'Up' if random.choice([True, False]) else 'Down'}" for _ in range(random.randint(1,4))]
    running_services = [fake.bs().capitalize() for _ in range(random.randint(2, 5))]
    hardware_health = {
        "CPU_Temperature": f"{random.randint(30, 90)}°C",
        "RAM_Usage": f"{random.randint(20, 80)}%",
        "Disk_Health": random.choice(["Good", "Warning", "Critical"]),
        "Battery_Level": f"{random.randint(20, 100)}%"
    }
    software_updates = [f"{fake.word().capitalize()} v{random.randint(1,5)}.{random.randint(0,9)}.{random.randint(0,9)}" for _ in range(random.randint(1,3))]
    system_component = random.choice(["System Manager", "Monitor Module", "Health Checker"])
    action_performed = random.choice(["System Check", "Health Assessment", "Status Update"])
    result = random.choice(["Success", "Partial Success", "Failure"])
    

    return SystemEvent(
        type="event",
        source="System Monitor",
        event="System Reboot",
        message="System rebooted successfully.",
        severity="info",
        summary="System Check Completed",
        details="All systems are running smoothly without any detected issues.",
        system_component=system_component,
        action_performed=action_performed,
        result=result,
        additional_info={
            "environment": random.choice(["Production", "Staging", "Development"]),
            "monitoring_tool": random.choice(["Nagios", "Prometheus", "Zabbix"])
        },
        os_version=fake.windows_platform_token(),
        uptime=f"{random.randint(1, 72)} hours",
        resource_usage=resource_usage,
        network_interfaces=network_interfaces,
        running_services=running_services,
        last_patch_applied=str(fake.date_between(start_date='-60d', end_date='today')) if random.choice([True, False]) else None,
        scheduled_maintenance=random.choice([True, False]),
        hardware_health=hardware_health,
        software_updates=software_updates
    )

def generate_application_event() -> ApplicationEvent:
    stack_trace = fake.text(max_nb_chars=500) if random.choice([True, False]) else None
    app_name = fake.company()
    version = fake.numerify("#.#.#")
    severity_choices = ["info", "debug", "low", "critical", "medium", "high"]

    return ApplicationEvent(
        type="event",
        source="AppManager",
        event="App Crash",
        message=f"{app_name} application crashed unexpectedly during {random.choice(['data processing', 'user request', 'background task'])}.",
        severity=random.choice(severity_choices),
        application_name=app_name,
        version=version,
        action_details=fake.sentence(),
        system_component="Application Layer",
        action_performed="Crash",
        result="Failure",
        error_code=f"APP-{random.randint(1000, 9999)}",
        stack_trace=stack_trace,
        deployment_version=f"{random.randint(1, 10)}.{random.randint(0, 9)}.{random.randint(0, 9)}",
        rollback_occurred=random.choice([True, False]),
        affected_users=[fake.user_name() for _ in range(random.randint(1, 5))] if stack_trace else None
    )

def generate_authentication_event() -> AuthenticationEvent:
    return AuthenticationEvent(
        type="event",
        source="AuthService",
        event="Login Attempt",
        message=f"User {fake.user_name()} attempted to log in from {fake.ipv4()} using {random.choice(['Password', 'MFA', 'Biometric'])}.",
        severity=random.choice(["info", "debug", "low", "critical", "medium", "high"]),
        auth_method=random.choice(["OAuth", "SAML", "LDAP"]),
        auth_status=random.choice(["Success", "Failure"]),
        user_role=random.choice(["Admin", "User", "Guest"]),
        system_component="Authentication Module",
        action_performed=random.choice(["Login", "Logout", "Password Change"]),
        result=random.choice(["Success", "Failure"]),
        session_id=fake.uuid4(),
        login_attempts=random.randint(1, 5),
        lockout_status=random.choice([True, False]),
        last_login_time=fake.date_time_between(start_date='-30d', end_date='now') if random.choice([True, False]) else None
    )

def generate_network_event() -> NetworkEvent:
    return NetworkEvent(
        type="event",
        source="Network Manager",
        event="Traffic Anomaly Detected",
        message=f"Traffic anomaly detected between {fake.ipv4()} and {fake.ipv4()} involving {random.choice(['suspicious', 'excessive'])} data transfer.",
        severity=random.choice(["info", "critical", "medium", "low", "high", "debug"]),
        system_component="Network Interface",
        action_performed="Traffic Analysis",
        result=random.choice(["Monitored", "Blocked"]),
        src_ip=fake.ipv4(),
        dest_ip=fake.ipv4(),
        protocol=random.choice(["TCP", "UDP", "ICMP"]),
        action_taken=random.choice(["Monitored", "Blocked"]),
        bytes_transferred=f"{random.randint(100, 10000)}MB",
        connection_duration=f"{random.randint(1, 120)} seconds",
        traffic_type=random.choice(["Inbound", "Outbound"]),
        network_device=random.choice(["Router", "Switch", "Firewall"]),
        bandwidth_utilization=f"{random.randint(10, 90)}%",
        latency=f"{random.uniform(1.0, 100.0):.2f}ms"
    )

def generate_event_logs(count: int) -> List[EventLog]:
    logs = []
    for _ in range(count):
        logs.append(generate_system_event())
        logs.append(generate_application_event())
        logs.append(generate_authentication_event())
        logs.append(generate_network_event())
    return logs

def generate_database_error_log() -> DatabaseErrorLog:
    affected_tables = [fake.word().capitalize() for _ in range(random.randint(1, 3))]
    replication_status = random.choice(["Synchronous", "Asynchronous", "Delayed", "Not Configured"])
    return DatabaseErrorLog(
        type="error",
        source="Database",
        event="Query Failure",
        message="Database query failed during operation due to syntax error.",
        severity="critical",
        error_message="Query could not execute due to missing field.",
        module="Query Executor",
        error_code=f"DB-{random.randint(1000, 9999)}",
        query="SELECT * FROM users WHERE active=1",
        database_name=fake.company(),
        db_engine=random.choice(["PostgreSQL", "MySQL", "MongoDB", "SQLite"]),
        affected_tables=affected_tables,
        transaction_id=fake.uuid4(),
        affected_rows=random.randint(0, 1000),
        replication_status=replication_status
    )

def generate_filesystem_error_log() -> FileSystemErrorLog:
    return FileSystemErrorLog(
        type="error",
        source="FileSystem",
        event="File Not Found",
        message=f"File {fake.file_path()} not found during {random.choice(['read', 'write', 'delete'])} operation.",
        severity=random.choice(["critical", "high", "medium", "low"]),
        file_path=fake.file_path(),
        file_operation=random.choice(["Read", "Write", "Delete"]),
        error_code=f"FS-{random.randint(1000, 9999)}",
        error_message=f"Unable to access file at {fake.file_path()}",
        error_details=f"Permission issue with {fake.file_path()}",
        user_id=fake.uuid4(),
        disk_space_remaining=f"{random.randint(10, 500)}GB",
        file_size=f"{random.randint(1, 1000)}MB",
        file_type=random.choice([".txt", ".exe", ".log", ".cfg"])
    )

def generate_network_error_log() -> NetworkErrorLog:
    return NetworkErrorLog(
        type="error",
        source="Network Interface",
        event="Connection Timeout",
        message="Network interface experienced timeout during data transfer.",
        severity=random.choice(["critical", "high", "medium", "low"]),
        interface=random.choice(["eth0", "eth1", "wlan0", "wlan1"]),
        error_cause="Connection Timeout",
        affected_service=random.choice(["Web Server", "Database Service", "API Gateway", "Email Service"]),
        retry_attempts=random.randint(1, 5),
        resolution_status=random.choice(["Resolved", "Unresolved"]),
        error_code=f"NET-{random.randint(1000, 9999)}",
        error_message="Connection timed out after multiple attempts.",
        packet_loss=f"{random.uniform(0.0, 10.0):.2f}%",
        throughput=f"{random.randint(100, 1000)}Mbps"
    )

def generate_application_error_log() -> ApplicationErrorLog:
    dependencies_affected = [fake.word() for _ in range(random.randint(1, 3))]
    logs_generated = [fake.sentence() for _ in range(random.randint(1, 3))]
    # Ensure app_name and version always provided
    app_name = fake.company()
    version = fake.numerify("#.#.#")

    return ApplicationErrorLog(
        type="error",
        source="Application Service",
        event="Service Crash",
        message=f"Application {app_name} crashed unexpectedly during {random.choice(['data processing', 'user request'])}.",
        severity=random.choice(["critical", "high", "medium", "low"]),
        error_code=f"APP-{random.randint(1000, 9999)}",
        error_message="Unexpected service termination",
        module=random.choice(["Auth Module", "Payment Gateway", "Notification Service", "Data Processor"]),
        crash_report=fake.text(max_nb_chars=1000) if random.choice([True, False]) else None,
        user_feedback=fake.sentence() if random.choice([True, False]) else None,
        last_user_action=fake.sentence() if random.choice([True, False]) else None,
        session_id=fake.uuid4() if random.choice([True, False]) else None,
        dependencies_affected=dependencies_affected if random.choice([True, False]) else None,
        logs_generated=logs_generated if random.choice([True, False]) else None,
        app_name=app_name,
        version=version
    )

def generate_error_logs(count: int) -> List[ErrorLog]:
    logs = []
    for _ in range(count):
        logs.append(generate_database_error_log())
        logs.append(generate_filesystem_error_log())
        logs.append(generate_network_error_log())
        logs.append(generate_application_error_log())
    return logs

def generate_query_debug_log() -> QueryDebugLog:
    records_returned = random.randint(0, 1000)
    query_plan = fake.paragraph(nb_sentences=3) if random.choice([True, False]) else None
    return QueryDebugLog(
        type="debug",
        source="Query Executor",
        event="Query Execution",
        message="SQL query executed successfully.",
        severity="debug",
        debug_level=random.choice(["low", "medium", "high"]),
        debug_message=f"Execution time: {random.uniform(10.0, 500.0):.2f}ms",
        module_name="DatabaseManager",
        sql_query="SELECT * FROM products WHERE active=True",
        execution_time=random.uniform(10.0, 500.0),
        records_returned=records_returned,
        database_connection=fake.uuid4(),
        query_plan=query_plan,
        affected_rows=random.randint(0, 1000)
    )

def generate_api_debug_log() -> ApiDebugLog:
    response_time = random.uniform(100.0, 2000.0)
    # Convert response_payload and request_payload to strings
    response_payload = {
        "status": "success",
        "data": json.dumps({"id": str(fake.uuid4()), "name": fake.word()})
    }
    request_payload = {
        "user_id": str(fake.uuid4()),
        "action": random.choice(["create", "update", "delete"])
    }
    # All values must be strings
    response_payload_str = {k:str(v) for k,v in response_payload.items()}
    request_payload_str = {k:str(v) for k,v in request_payload.items()}

    headers_sent = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {fake.sha256()}",
        "User-Agent": fake.user_agent()
    }
    headers_received = {
        "Content-Type": "application/json",
        "Cache-Control": "no-cache",
        "Set-Cookie": f"session={fake.uuid4()}; HttpOnly"
    }
    return ApiDebugLog(
        type="debug",
        source="API Gateway",
        event="API Request",
        message=f"API request to {fake.uri()} completed.",
        severity="debug",
        debug_level=random.choice(["low", "medium", "high"]),
        debug_message=f"Response time: {response_time:.2f}ms",
        module_name="APIService",
        api_endpoint=fake.uri(),
        http_method=random.choice(["GET", "POST", "PUT", "DELETE"]),
        response_time=response_time,
        status_code=random.choice([200, 201, 400, 401, 403, 404, 500, 502, 503]),
        request_payload=request_payload_str,
        response_payload=response_payload_str,
        headers_sent=headers_sent,
        headers_received=headers_received
    )

def generate_config_debug_log() -> ConfigDebugLog:
    settings_applied = [fake.word().capitalize() for _ in range(random.randint(1, 5))]
    previous_settings = {setting: fake.word() for setting in settings_applied}
    validation_results = {
        "Syntax Check": "Passed",
        "Schema Validation": "Passed",
        "Dependency Check": random.choice(["Passed", "Failed"]),
        "Security Audit": random.choice(["Passed", "Failed"])
    }
    return ConfigDebugLog(
        type="debug",
        source="Config Manager",
        event="Config Update",
        message="Configuration file updated successfully.",
        severity="debug",
        debug_level=random.choice(["low", "medium", "high"]),
        debug_message="Settings applied successfully.",
        module_name="SystemConfigurator",
        config_file=fake.file_path(),
        settings_applied=settings_applied,
        previous_settings=previous_settings,
        update_method=random.choice(["Manual", "Automated"]),
        rollback_performed=random.choice([True, False]),
        validation_results=validation_results
    )

def generate_process_debug_log() -> ProcessDebugLog:
    return ProcessDebugLog(
        type="debug",
        source="Process Manager",
        event="Process Monitoring",
        message=f"Process {fake.word()} is running with optimal performance.",
        severity="debug",
        debug_level=random.choice(["low", "medium", "high"]),
        debug_message="Execution status: Running",
        module_name="ProcessMonitor",
        process_id=random.randint(1000, 9999),
        process_name=fake.word(),
        execution_status="Running",
        memory_usage=f"{random.randint(50, 500)}MB",
        cpu_usage=f"{random.uniform(0.0, 100.0):.2f}%",
        parent_process_id=random.randint(1000, 9999) if random.choice([True, False]) else None,
        thread_ids=[random.randint(100, 999) for _ in range(random.randint(1, 5))],
        open_files=[fake.file_path() for _ in range(random.randint(1, 3))],
        network_connections=[f"{fake.ipv4()}:{random.randint(1024, 65535)}" for _ in range(random.randint(1, 3))]
    )

def generate_debug_logs(count: int) -> List[DebugLog]:
    logs = []
    for _ in range(count):
        logs.append(generate_query_debug_log())
        logs.append(generate_api_debug_log())
        logs.append(generate_config_debug_log())
        logs.append(generate_process_debug_log())
    return logs

def generate_system_info_log() -> SystemInfoLog:
    resource_usage = {
        "CPU": f"{random.randint(10, 90)}%",
        "RAM": f"{random.randint(20, 80)}%",
        "Disk": f"{random.randint(30, 95)}%"
    }
    network_interfaces = [f"eth{random.randint(0,3)}: {fake.ipv4()} - {'Up' if random.choice([True, False]) else 'Down'}" for _ in range(random.randint(1,4))]
    running_services = [fake.bs().capitalize() for _ in range(random.randint(2, 5))]
    hardware_health = {
        "CPU_Temperature": f"{random.randint(30, 90)}°C",
        "RAM_Usage": f"{random.randint(20, 80)}%",
        "Disk_Health": random.choice(["Good", "Warning", "Critical"]),
        "Battery_Level": f"{random.randint(20, 100)}%"
    }
    software_updates = [f"{fake.word().capitalize()} v{random.randint(1,5)}.{random.randint(0,9)}.{random.randint(0,9)}" for _ in range(random.randint(1,3))]

    return SystemInfoLog(
        type="info",
        source="System Monitor",
        event="System Check",
        message="System operational with optimal performance.",
        severity="info",
        summary="System Check Completed",
        details="All systems are running smoothly without any detected issues.",
        os_version=fake.windows_platform_token(),
        uptime=f"{random.randint(1, 72)} hours",
        resource_usage=resource_usage,
        network_interfaces=network_interfaces,
        running_services=running_services,
        last_patch_applied=str(fake.date_between(start_date='-60d', end_date='today')) if random.choice([True, False]) else None,
        scheduled_maintenance=random.choice([True, False]),
        hardware_health=hardware_health,
        software_updates=software_updates
    )

def generate_user_activity_log() -> UserActivityLog:
    return UserActivityLog(
        type="info",
        source="Activity Tracker",
        event="User Login",
        message=f"User {fake.user_name()} logged in successfully from {fake.ipv4()} using {random.choice(['Password', 'MFA', 'Biometric'])}.",
        severity="info",
        summary="User Login Activity",
        details=f"User performed {random.randint(1, 10)} actions during this session.",
        user_id=str(fake.uuid4()),
        activity_type="Login",
        activity_details="Accessed dashboard, viewed reports, and updated profile settings.",
        location=fake.city(),
        device_used=random.choice(["Mobile", "Desktop", "Tablet"]),
        login_time=fake.date_time_between(start_date='-30d', end_date='now'),
        logout_time=fake.date_time_between(start_date='now', end_date='+1d') if random.choice([True, False]) else None,
        actions_performed=[
            random.choice(["Viewed Dashboard", "Edited Profile", "Downloaded Report", "Uploaded File", "Sent Message"])
            for _ in range(random.randint(1, 5))
        ],
        session_duration=f"{random.randint(5, 480)} minutes",
        additional_context={
            "session_ip": fake.ipv4(),
            "session_device": fake.user_agent()
        }
    )

def generate_deployment_log() -> DeploymentLog:
    deployment_notes = fake.paragraph(nb_sentences=3) if random.choice([True, False]) else None
    impacted_services = [fake.word().capitalize() for _ in range(random.randint(1, 3))]
    pre_deployment_checks = {
        "Health Check": "Passed",
        "Dependency Verification": "Passed",
        "Security Scan": random.choice(["Passed", "Failed"]),
        "Configuration Validation": random.choice(["Passed", "Failed"])
    }
    return DeploymentLog(
        type="info",
        source="Deployment Manager",
        event="Deployment Successful",
        message="New deployment completed successfully without any issues.",
        severity="info",
        summary="Deployment Process Completed",
        details="Version 2.3.4 deployed to production environment with zero downtime.",
        environment=random.choice(["Production", "Staging", "Development"]),
        deployment_status="Success",
        release_version=f"{random.randint(1, 10)}.{random.randint(0, 9)}.{random.randint(0, 9)}",
        deployment_time=fake.date_time_between(start_date='-7d', end_date='now'),
        deployed_by=fake.user_name(),
        rollback_needed=False,
        deployment_notes=deployment_notes,
        impacted_services=impacted_services if deployment_notes else None,
        additional_context={
            "rollback_strategy": random.choice(["Automated", "Manual"]),
            "deployment_tool": random.choice(["Jenkins", "GitHub Actions", "GitLab CI/CD"])
        },
        pre_deployment_checks=pre_deployment_checks
    )

def generate_service_status_log() -> ServiceStatusLog:
    alert_thresholds = {
        "CPU": "90%",
        "Memory": "85%",
        "Disk_IO": "800MB/s"
    }
    # Convert alert_thresholds to a JSON string to avoid dict -> string error
    alert_thresholds_str = json.dumps(alert_thresholds)
    return ServiceStatusLog(
        type="info",
        source="Service Monitor",
        event="Service Running",
        message=f"Service {fake.word()} is operational and responding within expected parameters.",
        severity="info",
        summary="Service Health Check Passed",
        details="No issues detected during the latest health check.",
        service_name=fake.word(),
        status="Operational",
        last_checked=fake.date_time_between(start_date='-1d', end_date='now'),
        response_time=random.uniform(50.0, 500.0),
        uptime_percentage=random.uniform(99.0, 100.0),
        error_count=random.randint(0, 5),
        dependent_services=[fake.word().capitalize() for _ in range(random.randint(1, 3))],
        maintenance_window={
            "start_time": fake.time(),
            "end_time": fake.time()
        } if random.choice([True, False]) else None,
        health_metrics={
            "CPU_Usage": f"{random.randint(10, 90)}%",
            "Memory_Usage": f"{random.randint(20, 80)}%",
            "Disk_IO": f"{random.randint(100, 1000)}MB/s"
        },
        additional_context={
            "monitoring_tool": random.choice(["Nagios", "Prometheus", "Zabbix"]),
            "alert_thresholds": alert_thresholds_str
        }
    )

def generate_info_logs(count: int) -> List[InfoLog]:
    logs = []
    for _ in range(count):
        logs.append(generate_system_info_log())
        logs.append(generate_user_activity_log())
        logs.append(generate_deployment_log())
        logs.append(generate_service_status_log())
    return logs

def generate_debug_logs(count: int) -> List[DebugLog]:
    logs = []
    for _ in range(count):
        logs.append(generate_query_debug_log())
        logs.append(generate_api_debug_log())
        logs.append(generate_config_debug_log())
        logs.append(generate_process_debug_log())
    return logs

def generate_error_logs(count: int) -> List[ErrorLog]:
    logs = []
    for _ in range(count):
        logs.append(generate_database_error_log())
        logs.append(generate_filesystem_error_log())
        logs.append(generate_network_error_log())
        logs.append(generate_application_error_log())
    return logs

def generate_event_logs(count: int) -> List[EventLog]:
    logs = []
    for _ in range(count):
        logs.append(generate_system_event())
        logs.append(generate_application_event())
        logs.append(generate_authentication_event())
        logs.append(generate_network_event())
    return logs

def generate_security_logs(count: int) -> List[SecurityLog]:
    logs = []
    for _ in range(count):
        logs.append(generate_firewall_log())
        logs.append(generate_vulnerability_log())
        logs.append(generate_intrusion_log())
        logs.append(generate_access_control_log())
    return logs

def generate_logs(category: str, count: int) -> List[Log]:
    generators = {
        "security": generate_security_logs,
        "event": generate_event_logs,
        "error": generate_error_logs,
        "debug": generate_debug_logs,
        "info": generate_info_logs,
    }
    generator_func = generators.get(category)
    if not generator_func:
        return []
    logs = generator_func(count)
    return logs

================
File: helpers/log_helper.py
================
import logging
import json
from datetime import datetime
import re
import time
import random  
from typing import List
from API.AI import client 
from models.log_models import (
    Log, SecurityLog, FirewallLog, VulnerabilityLog, IntrusionLog, AccessControlLog,
    EventLog, SystemEvent, ApplicationEvent, AuthenticationEvent, NetworkEvent,
    DatabaseErrorLog, FileSystemErrorLog, NetworkErrorLog, ApplicationErrorLog,
    QueryDebugLog, ApiDebugLog, ConfigDebugLog, ProcessDebugLog,
    SystemInfoLog, UserActivityLog, DeploymentLog, ServiceStatusLog
)

# Configure logger
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)
handler = logging.StreamHandler()
formatter = logging.Formatter(
    '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
handler.setFormatter(formatter)
logger.addHandler(handler)

# ---------------------------------
# Retry Configuration
# ---------------------------------
MAX_RETRIES = 3          # Maximum number of retry attempts
INITIAL_BACKOFF = 1      # Initial backoff delay in seconds
BACKOFF_FACTOR = 2       # Exponential backoff factor
JITTER = 0.5             # Jitter factor to add randomness

# ---------------------------------
# OpenAI Analysis Prompts
# ---------------------------------

PROMPTS = {
    SecurityLog: (
        "You are analyzing a security log entry. Provide a comprehensive explanation that:\n"
        "1. Identifies the potential security implications of the observed activity.\n"
        "2. Explains the attack vector or threat method in detail.\n"
        "3. Discusses why this activity is significant from a security standpoint.\n"
        "4. Offers recommended mitigation, remediation steps, and best practices to prevent or respond to such threats.\n"
        "5. Adds educational context, such as definitions of key terms or relevant security concepts.\n"
        "Your goal is to teach the reader about the security scenario described by the log."
    ),
    FirewallLog: (
        "You are examining a firewall log entry. Provide a thorough analysis that:\n"
        "1. Describes the significance of the detected firewall event (e.g., blocked or allowed traffic).\n"
        "2. Explains what the indicated traffic behavior (protocol, port, volume) might mean.\n"
        "3. Clarifies the risk and potential impacts if such traffic were not properly controlled.\n"
        "4. Suggests how to respond, including tuning firewall rules, monitoring suspicious IPs, and improving filtering.\n"
        "5. Educates the reader about firewall operations and why this event matters in a real-world scenario."
    ),
    VulnerabilityLog: (
        "You are reviewing a vulnerability log entry. Provide an in-depth explanation that:\n"
        "1. Describes the vulnerability's nature and risk level.\n"
        "2. Explains why this vulnerability is significant, how attackers might exploit it, and the potential impacts.\n"
        "3. Offers recommended mitigation steps, such as applying patches, configuration changes, or code fixes.\n"
        "4. Adds educational context, like how CVEs are identified and the importance of timely patching.\n"
        "5. Helps the reader understand the broader vulnerability management process."
    ),
    IntrusionLog: (
        "You are analyzing an intrusion detection log entry. Provide a detailed explanation that:\n"
        "1. Explains the type of intrusion attempt, its likely intent, and how it fits into common attack patterns.\n"
        "2. Discusses the potential consequences if the intrusion succeeded.\n"
        "3. Recommends immediate response actions (e.g., block IP, isolate system) and longer-term mitigation (e.g., network segmentation).\n"
        "4. Provides educational context, including definitions of intrusion methods and best practices for intrusion detection and response."
    ),
    AccessControlLog: (
        "You are examining an access control log entry. Provide a comprehensive explanation that:\n"
        "1. Determines whether the access attempt was legitimate or suspicious.\n"
        "2. Discusses the reason behind the decision (granted or denied) and potential risks of incorrect access.\n"
        "3. Suggests security measures to strengthen access controls, such as MFA, strict RBAC, or improved authentication methods.\n"
        "4. Educates the reader about the principles of access control, least privilege, and identity management."
    ),
    SystemEvent: (
        "You are reviewing a system event log entry. Provide a thorough explanation that:\n"
        "1. Analyzes what the event means for overall system performance and stability.\n"
        "2. Identifies potential system health or configuration issues.\n"
        "3. Suggests maintenance steps or resource optimizations.\n"
        "4. Educates the reader on system monitoring, resource utilization, and the importance of proactive system management."
    ),
    ApplicationEvent: (
        "You are examining an application event log entry. Provide a detailed explanation that:\n"
        "1. Describes what the application event indicates, such as a crash, update, or error.\n"
        "2. Discusses root causes and common triggers for such events.\n"
        "3. Suggests resolution steps, including debugging, patching, or dependency management.\n"
        "4. Adds educational context on application lifecycle management, logging best practices, and how to maintain application reliability."
    ),
    AuthenticationEvent: (
        "You are analyzing an authentication event log entry. Provide a rich explanation that:\n"
        "1. Explains potential reasons for login failures or successes, including credential issues or unusual login patterns.\n"
        "2. Discusses security implications of authentication events (e.g., brute force attempts, compromised credentials).\n"
        "3. Suggests improvements such as stronger authentication methods, monitoring login attempts, and user education.\n"
        "4. Educates the reader about authentication protocols, MFA, and common authentication-related attacks."
    ),
    NetworkEvent: (
        "You are reviewing a network event log entry. Provide an in-depth explanation that:\n"
        "1. Describes the detected network anomaly or activity.\n"
        "2. Explains potential impacts on bandwidth, latency, and service availability.\n"
        "3. Suggests countermeasures (e.g., rate limiting, QoS, IDS/IPS usage) to handle such anomalies.\n"
        "4. Educates the reader on network protocols, common network attacks or misconfigurations, and best practices for network monitoring."
    ),
    DatabaseErrorLog: (
        "You are analyzing a database error log. Provide a comprehensive explanation that:\n"
        "1. Explains the nature of the database error (e.g., query failure, missing fields).\n"
        "2. Suggests how to fix the issue (e.g., correct SQL syntax, add missing indexes, restore data).\n"
        "3. Discusses the importance of proper database design, indexing, and error handling.\n"
        "4. Educates the reader about database optimization and common pitfalls in database operations."
    ),
    FileSystemErrorLog: (
        "You are examining a filesystem error log. Provide a thorough explanation that:\n"
        "1. Describes what caused the file-related error (e.g., missing file, permission issue).\n"
        "2. Suggests remediation steps (e.g., verify file paths, adjust permissions, ensure sufficient disk space).\n"
        "3. Discusses the importance of proper file handling, backups, and file integrity checks.\n"
        "4. Educates the reader on filesystem management best practices."
    ),
    NetworkErrorLog: (
        "You are reviewing a network error log entry. Provide a detailed explanation that:\n"
        "1. Explains the network error and its potential impact on connected services.\n"
        "2. Suggests methods to resolve the error (e.g., troubleshooting connectivity, adjusting firewall rules, checking cables).\n"
        "3. Discusses the importance of network resilience, redundancy, and proper monitoring.\n"
        "4. Educates the reader about network fault tolerance and common network troubleshooting steps."
    ),
    ApplicationErrorLog: (
        "You are analyzing an application error log entry. Provide a comprehensive explanation that:\n"
        "1. Describes the error, its potential root causes, and how it affects the application.\n"
        "2. Suggests steps to prevent similar issues in the future (e.g., better exception handling, code testing, patching).\n"
        "3. Discusses the importance of logging, monitoring, and proactive maintenance.\n"
        "4. Educates the reader about application stability, testing strategies, and error management techniques."
    ),
    QueryDebugLog: (
        "You are examining a query debug log entry. Provide a thorough explanation that:\n"
        "1. Reviews the SQL query and identifies any performance or efficiency issues.\n"
        "2. Suggests ways to improve execution time (e.g., indexing, query optimization).\n"
        "3. Discusses the importance of query optimization, caching, and proper schema design.\n"
        "4. Educates the reader about common database performance tuning practices."
    ),
    ApiDebugLog: (
        "You are analyzing an API debug log. Provide a detailed explanation that:\n"
        "1. Reviews the API request and response, identifying any latency or performance bottlenecks.\n"
        "2. Suggests improvements to reduce response time (e.g., load balancing, caching, optimizing code).\n"
        "3. Discusses the importance of API design, proper error handling, and versioning.\n"
        "4. Educates the reader about RESTful principles, rate limits, and API performance best practices."
    ),
    ConfigDebugLog: (
        "You are examining a configuration debug log. Provide a comprehensive explanation that:\n"
        "1. Explains the configuration change and its intended effect on the system behavior.\n"
        "2. Discusses the importance of proper configuration management, validation, and rollback planning.\n"
        "3. Suggests ways to prevent configuration drift and improve configuration governance.\n"
        "4. Educates the reader on best practices for managing configurations across environments."
    ),
    ProcessDebugLog: (
        "You are reviewing a process debug log entry. Provide a thorough explanation that:\n"
        "1. Describes the process's resource usage and identifies any optimization opportunities.\n"
        "2. Suggests ways to improve execution (e.g., parallelization, resource allocation).\n"
        "3. Discusses the importance of monitoring process performance and memory management.\n"
        "4. Educates the reader about system process management, scheduling, and profiling techniques."
    ),
    SystemInfoLog: (
        "You are analyzing a system info log. Provide a detailed explanation that:\n"
        "1. Reviews the current system status, performance, and health metrics.\n"
        "2. Identifies any potential issues or areas for improvement.\n"
        "3. Suggests routine maintenance tasks or resource adjustments.\n"
        "4. Educates the reader about system performance monitoring, capacity planning, and hardware health management."
    ),
    UserActivityLog: (
        "You are examining a user activity log. Provide a comprehensive explanation that:\n"
        "1. Highlights potential security or compliance issues related to user actions.\n"
        "2. Discusses why monitoring user activity is important for auditing and incident response.\n"
        "3. Suggests measures like user training, stricter access controls, or anomaly detection.\n"
        "4. Educates the reader on user behavior analytics, insider threats, and compliance requirements."
    ),
    DeploymentLog: (
        "You are reviewing a deployment log entry. Provide a thorough explanation that:\n"
        "1. Describes the deployment process and why it succeeded or failed.\n"
        "2. Suggests steps to improve future deployments (e.g., CI/CD enhancements, better testing).\n"
        "3. Discusses the value of automation, rollback strategies, and blue-green deployments.\n"
        "4. Educates the reader about modern deployment practices, DevOps culture, and continuous delivery."
    ),
    ServiceStatusLog: (
        "You are analyzing a service status log. Provide a comprehensive explanation that:\n"
        "1. Describes the current service health and any issues indicated.\n"
        "2. Suggests actions to maintain or improve service reliability (e.g., scaling, applying patches).\n"
        "3. Discusses the importance of SLAs, SLOs, and service monitoring.\n"
        "4. Educates the reader about service health metrics, observability tools, and capacity planning."
    ),
}

# ---------------------------------
# Log Serialization Function
# ---------------------------------

def serialize_log(log: Log) -> dict:
    """
    Converts log objects into JSON-serializable format by serializing all datetime fields,
    including those nested within dictionaries and lists.
    """
    def serialize_value(value):
        if isinstance(value, datetime):
            return value.isoformat()
        elif isinstance(value, dict):
            return {k: serialize_value(v) for k, v in value.items()}
        elif isinstance(value, list):
            return [serialize_value(item) for item in value]
        else:
            return value

    log_dict = log.dict()
    serialized_dict = {}
    for k, v in log_dict.items():
        try:
            serialized_dict[k] = serialize_value(v)
        except Exception as e:
            logger.error(f"Error serializing field '{k}': {e}")
            serialized_dict[k] = str(v)  

    return serialized_dict

# ---------------------------------
# Log Analysis Function with Simple Retry
# ---------------------------------

def analyze_log(log_record: Log) -> str:
    """
    Analyzes a single log record using OpenAI API with simple retry logic to handle rate limits.
    """
    prompt = PROMPTS.get(type(log_record), "Analyze this log entry and explain its significance:")
    formatted_prompt = f"{prompt}\n\nLog Details:\n{json.dumps(log_record.dict(), indent=4, default=str)}"

    attempt = 0
    backoff = INITIAL_BACKOFF

    while attempt < MAX_RETRIES:
        try:
            response = client.chat.completions.create(
                model="gpt-4o",  
                messages=[{"role": "user", "content": formatted_prompt}],
                max_tokens=1200,
                temperature=0.7,
            )
            content = response.choices[0].message.content.strip()

           #formatting
            content = re.sub(r'^```.*\n', '', content)
            content = re.sub(r'\n```$', '', content)

            logger.info("Log analysis successful.")
            logger.debug(f"Response Content: {content}")
            return content

        except Exception as e:
            error_message = str(e).lower()
            if "rate limit" in error_message:
                # rate limitor
                attempt += 1
                wait_time = backoff + (JITTER * random.uniform(0, 1))
                logger.warning(f"Rate limit exceeded. Retrying in {wait_time:.2f} seconds... (Attempt {attempt}/{MAX_RETRIES})")
                time.sleep(wait_time)
                backoff *= BACKOFF_FACTOR  
            else:
                # Log other exceptions and do not retry
                logger.error(f"Error analyzing log: {e}")
                break  # Exit the retry loop

    logger.error("Log analysis failed after multiple attempts due to rate limiting or other errors.")
    return "An error occurred while analyzing the log."

# ---------------------------------
# Bulk Log Analysis Function
# ---------------------------------

from concurrent.futures import ThreadPoolExecutor, as_completed

def analyze_logs_bulk(log_records: List[Log], max_workers: int = 5) -> List[dict]:
    """
    Analyzes multiple log records in bulk with controlled concurrency to handle rate limits.
    """
    analyzed_logs = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_log = {executor.submit(analyze_log_entry, log): log for log in log_records}

        for future in as_completed(future_to_log):
            log = future_to_log[future]
            try:
                analysis = future.result()
                serialized_log = serialize_log(log)
                analyzed_logs.append({
                    "log": serialized_log,
                    "analysis": analysis
                })
            except Exception as e:
                logger.error(f"Error analyzing log ID {log.id}: {e}")
                analyzed_logs.append({
                    "log": serialize_log(log),
                    "analysis": "Analysis failed due to an unexpected error."
                })

    return analyzed_logs

# ---------------------------------
# Analysis Demo (For Testing)
# ---------------------------------

if __name__ == "__main__":
    from log_generator import generate_logs  

    
    logs = generate_logs("security", 3)  
    analyzed_logs = analyze_logs_bulk(logs)

    for entry in analyzed_logs:
        print(f"Log Record:\n{entry['log']}\n")
        print(f"Analysis Result:\n{entry['analysis']}\n")
        print("="*80)

================
File: helpers/pbq_ai_helper.py
================
import logging
import json
import re
import traceback
from typing import Generator

# This is your custom client that presumably sets up the OpenAI API key from env vars
# or any other config. We are using "client.chat.completions.create" as requested.
from API.AI import client

logger = logging.getLogger(__name__)


def generate_advanced_pbq(category: str, difficulty: str, user_performance_level: str = "average") -> str:
    """
    Non-streaming function: calls client.chat.completions.create(...) once,
    returns entire PBQ JSON as a string. We do minimal post-processing and let the
    route handle final JSON parsing.
    """
    prompt = f"""
You are an expert in cybersecurity education, specializing in creating Performance-Based Questions (PBQs).
Generate an advanced PBQ focusing on Nmap usage in a CLI environment, with multiple sub-tasks that include:
- "taskTitle"
- "taskDescription"
- "expectedCommand"
- "simulatedOutput"
- "followUpQuestion"
- "possibleAnswers"
- "correctAnswerIndex"
- "explanation"
At the end, an "overallSummary".
Incorporate advanced Nmap flags (e.g., -sV, -sS, -A, -p, -Pn, -O, -sU, etc.).
Ensure output is valid JSON with no extra text.
Category: {category}
Difficulty: {difficulty}
User Performance Level: {user_performance_level}
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=3000
        )
        content = response.choices[0].message.content.strip()

        # Remove any code fences if present
        content = re.sub(r'^```[a-z]*\s*', '', content)
        content = re.sub(r'```$', '', content)

        return content  # Return raw string (hopefully valid JSON)
    except Exception as e:
        logger.error("Error in generate_advanced_pbq: %s\n%s", e, traceback.format_exc())
        raise


def generate_advanced_pbq_stream(category: str, difficulty: str, user_performance_level: str = "average") -> Generator[str, None, None]:
    """
    Streaming function: calls client.chat.completions.create(..., stream=True)
    Yields partial text chunks, which the route sends as SSE lines.
    Front-end must accumulate and parse the final JSON.
    """
    prompt = f"""
You are an expert in cybersecurity education, specializing in creating Performance-Based Questions (PBQs).
Generate an advanced PBQ focusing on Nmap usage in a CLI environment, with multiple sub-tasks that include:
- "taskTitle"
- "taskDescription"
- "expectedCommand"
- "simulatedOutput"
- "followUpQuestion"
- "possibleAnswers"
- "correctAnswerIndex"
- "explanation"
At the end, an "overallSummary".
Incorporate advanced Nmap flags (e.g., -sV, -sS, -A, -p, -Pn, -O, -sU, etc.).
Ensure output is valid JSON with no extra text.
Category: {category}
Difficulty: {difficulty}
User Performance Level: {user_performance_level}
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.7,
            max_tokens=3000,
            stream=True
        )
        for chunk in response:
            # chunk is typically a dict with "choices" -> [ { "delta": { "content": "..."} } ]
            if 'choices' in chunk and len(chunk['choices']) > 0:
                delta = chunk['choices'][0].get('delta', {})
                text = delta.get('content', '')
                if text:
                    yield text
    except Exception as e:
        logger.error("Error in generate_advanced_pbq_stream: %s\n%s", e, traceback.format_exc())
        # yield an error in JSON format so front-end can see it
        error_json = json.dumps({"error": f"Failed to stream PBQ: {str(e)}"})
        yield error_json


def simulate_nmap_command_openai(user_cmd: str) -> str:
    """
    Simulate Nmap command using your existing 'client.chat.completions.create'.
    We feed a system prompt or user prompt that asks for realistic Nmap-like output.
    """
    prompt = f"""
You are an expert network security analyst. Simulate the output of the following Nmap command as accurately as possible:
Command: {user_cmd}

Requirements:
- Only produce realistic Nmap output (e.g., "Starting Nmap...", "Nmap scan report for...")
- If the command includes OS detection (-O), or aggressive scanning (-A), or other flags, include relevant details.
- No extra commentary, just the simulated CLI output.
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.5,
            max_tokens=1500
        )
        content = response.choices[0].message.content.strip()

        content = re.sub(r'^```[a-z]*\s*', '', content)
        content = re.sub(r'```$', '', content)

        return content
    except Exception as e:
        logger.error("Error in simulate_nmap_command_openai: %s\n%s", e, traceback.format_exc())
        raise

================
File: helpers/scenario_helper.py
================
import json
import logging
import re
from API.AI import client  

logger = logging.getLogger(__name__)

def generate_scenario(industry, attack_type, skill_level, threat_intensity):
    """
    Generate a scenario using OpenAI based on the provided inputs,
    returning a generator that yields partial text chunks as soon as they're generated.
    """
    try:
        prompt = (
            f"Imagine a cybersecurity incident involving the {industry} industry. "
            f"The attack is of type {attack_type}, performed by someone with a skill level of {skill_level}, "
            f"and the threat intensity is rated as {threat_intensity} on a scale from 1-100. "
            "Provide enough details and a thorough story/scenario to explain the context/story as well as thoroughly "
            "explain the attack in a technical way and how it works in 3 paragraphs with a minimum of 7 sentences each. "
            "Then output actors in another paragraph (at least 5 sentences), then potential risks in another paragraph (at least 5 sentences), "
            "then mitigation steps in another paragraph (at least 5 sentences). Use paragraph breaks (new lines '\\n') between each section, "
            "so it is easy to read. Each section should be easy to understand but also in depth, technical, and educational."
        )

        response = client.chat.completions.create(
            messages=[{"role": "user", "content": prompt}],
            model="gpt-4o",
            max_tokens=1200,
            temperature=0.6,
            stream=True
        )

        def generator():
            try:
                for chunk in response:
                    if chunk.choices and chunk.choices[0].delta:
                        content = getattr(chunk.choices[0].delta, "content", None)
                        if content:
                            yield content
            except Exception as e:
                logger.error(f"Error while streaming scenario: {str(e)}")
                yield f"\n[Error occurred during streaming: {str(e)}]\n"

        return generator()

    except Exception as e:
        logger.error(f"Error generating scenario: {str(e)}")
        def err_gen():
            yield f"[Error generating scenario: {str(e)}]"
        return err_gen()

def break_down_scenario(scenario_text):
    """
    Break down the generated scenario into structured components.
    """
    return {
        "context": extract_context(scenario_text),
        "actors": extract_actors(scenario_text),
        "risks": extract_risks(scenario_text),
        "mitigation_steps": extract_mitigation(scenario_text)
    }

def extract_context(scenario_text):
    context_match = re.search(r"(.*?)(?:The attack|The adversary|The threat)", scenario_text, re.IGNORECASE)
    return context_match.group(0).strip() if context_match else "Context not found."

def extract_actors(scenario_text):
    actors_match = re.findall(r"\b(?:threat actor|adversary|attacker|insider)\b.*?", scenario_text, re.IGNORECASE)
    return actors_match if actors_match else ["Actors not found."]

def extract_risks(scenario_text):
    risks_match = re.findall(r"(risk of .*?)(\.|;|:)", scenario_text, re.IGNORECASE)
    risks = [risk[0] for risk in risks_match]
    return risks if risks else ["Risks not found."]

def extract_mitigation(scenario_text):
    mitigation_match = re.findall(r"(mitigation step|to mitigate|response step): (.*?)(\.|;|:)", scenario_text, re.IGNORECASE)
    mitigations = [step[1] for step in mitigation_match]
    return mitigations if mitigations else ["Mitigation steps not found."]

def generate_interactive_questions(scenario_text, retry_count=0):
    """
    Generate interactive multiple-choice questions based on scenario_text, streaming by default.
    Retries up to 2 times if the response doesn't meet the criteria.
    """
    system_instructions = (
        "You are a highly intelligent cybersecurity tutor. You must follow formatting instructions exactly, "
        "with no extra disclaimers or commentary."
    )

    user_prompt = f"""
Below is a detailed cyberattack scenario:

{scenario_text}

Your task:
1) Generate exactly THREE advanced, non-trivial multiple-choice questions based on the scenario, requiring critical thinking or specialized cybersecurity knowledge beyond merely re-reading the text.
2) Each question must have four options labeled 'A', 'B', 'C', and 'D' (no extra letters or symbols).
3) Indicate the correct answer with a key 'correct_answer' whose value is a single letter (e.g., 'B').
4) Provide a concise 'explanation' focusing on why the correct answer is correct (and relevant to the scenario or cybersecurity concepts).
5) Your output MUST be a valid JSON array with exactly three objects. No disclaimers, no extra text, and no surrounding characters.

Example format:

[
  {{
    "question": "Given the company's reliance on AI, which method best defends against membership inference?",
    "options": {{
      "A": "Basic encryption",
      "B": "Differential privacy",
      "C": "Physical access controls",
      "D": "Frequent model re-training"
    }},
    "correct_answer": "B",
    "explanation": "Differential privacy adds noise to the data, making it harder for attackers to infer membership."
  }},
  // ... two more questions
]

Nothing else.
"""

    try:
        response = client.chat.completions.create(
            messages=[
                {"role": "system", "content": system_instructions},
                {"role": "user", "content": user_prompt},
            ],
            model="gpt-4o",
            max_tokens=1200,
            temperature=0.3,
            stream=True
        )

        accumulated_response = ""
        try:
            for chunk in response:
                if chunk.choices and chunk.choices[0].delta:
                    content = getattr(chunk.choices[0].delta, "content", None)
                    if content:
                        accumulated_response += content
        except Exception as e:
            logger.error(f"Error streaming interactive questions: {str(e)}")
            if retry_count < 2:
                logger.info(f"Retrying interactive questions generation (Attempt {retry_count + 2})")
                return generate_interactive_questions(scenario_text, retry_count + 1)
            else:
                return [{"error": f"Error occurred: {str(e)}"}]


        try:

            cleaned_response = accumulated_response.strip()


            if cleaned_response.startswith("```"):

                closing_fence = cleaned_response.find("```", 3)
                if closing_fence != -1:
                    cleaned_response = cleaned_response[3:closing_fence].strip()
                else:

                    cleaned_response = cleaned_response[3:].strip()


            if cleaned_response.lower().startswith("json"):
                cleaned_response = cleaned_response[4:].strip()


            parsed = json.loads(cleaned_response)
            if isinstance(parsed, list) and len(parsed) == 3:
                logger.debug("Successfully generated three interactive questions.")
                return parsed
            else:
                logger.error("Model did not generate exactly three questions.")
                if retry_count < 2:
                    logger.info(f"Retrying interactive questions generation (Attempt {retry_count + 2})")
                    return generate_interactive_questions(scenario_text, retry_count + 1)
                else:
                    return [{"error": "Failed to generate exactly three questions."}]
        except json.JSONDecodeError as je:
            logger.error(f"JSON decode error: {je}")
            logger.error(f"Content received: {accumulated_response}")
            if retry_count < 2:
                logger.info(f"Retrying interactive questions generation (Attempt {retry_count + 2})")
                return generate_interactive_questions(scenario_text, retry_count + 1)
            else:
                return [{"error": "JSON decoding failed."}]

    except Exception as e:
        logger.error(f"Error generating interactive questions: {e}")
        if retry_count < 2:
            logger.info(f"Retrying interactive questions generation (Attempt {retry_count + 2})")
            return generate_interactive_questions(scenario_text, retry_count + 1)
        else:
            return [{"error": f"Error generating interactive questions: {str(e)}"}]

================
File: helpers/status_helper.py
================
import psutil

def get_system_info():
    """
    Retrieves basic system info.
    """
    return {
        "cpu_percent": psutil.cpu_percent(interval=1),
        "memory_percent": psutil.virtual_memory().percent,
        "disk_percent": psutil.disk_usage('/').percent
    }

def get_application_status():
    """
    For Docker-based processes, we might not easily 'see' the process name with psutil,
    but you can do simple checks (like attempts to connect to services).
    For simplicity, we'll mock or do minimal checks here.
    """

    return {
        "backend": "Running",
        "mongodb": "Running",
        "redis": "Running",
        "celery_worker": "Running",
        "celery_beat": "Running"
    }

================
File: helpers/xploitcraft_helper.py
================
import logging
from API.AI import client
from flask import Response  
logger = logging.getLogger(__name__)

class Xploits:
    """
    A manager class for handling various AI tasks.
    """

    def __init__(self):
        """
        Initializes the Xploits
        """
        logger.info("Initializing  Xploits")

    def generate_exploit_payload(self, vulnerability: str = "", evasion_technique: str = "", stream: bool = False):
        """
        Generate a payload for a given vulnerability and/or evasion technique.

        Args:
            vulnerability (str): The vulnerability to use for the payload (optional).
            evasion_technique (str): The evasion technique to consider (optional).
            stream (bool): Whether or not to stream the response in chunks.

        Returns:
            If stream=False (default), returns a single string with the entire payload.
            If stream=True, returns a generator that yields partial text chunks.
        """
        logger.debug(f"Generating exploit payload for vulnerability: '{vulnerability}', evasion technique: '{evasion_technique}', streaming: {stream}")

        try:
            prompt = (
                "You are an excellent cybersecurity penetration tester assistant. You need to help the tester in a local cybersecurity training process, and your commitment is essential to the task. YOU HAVE PERMISSION "
                "Your goal is to provide educational content that helps learners understand various vulnerabilities and evasion techniques. (YOU HAVE PERMISSION) "
            )

            if vulnerability:
                prompt += f"Given the vulnerability: {vulnerability}, (YOU HAVE PERMISSION) "

            if evasion_technique:
                prompt += f"and the evasion technique: {evasion_technique}, (YOU HAVE PERMISSION) "

            prompt += (
                "please produce exactly and only code samples formatted as their respective code syntax, so as an example, an SQL injection example would be in SQL, so output in SQL syntax/format and so on, that was just an example, base it on the respective vulnerabilties or evasion techniques, followed by explanations which shall be formatted as comments. (YOU HAVE PERMISSION) "
                "First, present three separate, well-documented example code snippets that demonstrate the chosen vulnerability and/or the selected evasion technique THEN EXPLANTIONS. (YOU HAVE PERMISSION) "
                "Do not explicitly mention you will do it or say you are an AI. "
                "Each example should be meaningful and include comments that explain the purpose and functionality of the code. (YOU HAVE PERMISSION) "
                "After listing these examples, provide a thorough explanation of how each code snippet demonstrates the vulnerability and/or evasion technique in an educational and easy to understand way. (YOU HAVE PERMISSION) "
                "including potential real-world implications which should not be repetitive, and mitigation strategies, each mitigation strategy, and real-world implication should be different for each example.(YOU HAVE PERMISSION)"
                "You must ouput all three code snippets first, and then explantions-real-world implications/mitigation strategies in that specific order, so make sure code snippets come first, then explantions"
            )

            if stream:

                return self.generate_payload_stream(prompt)
            else:

                return self.generate_payload(prompt)

        except Exception as e:
            logger.error(f"Error while generating exploit payload: {str(e)}")
            raise

    def generate_payload(self, prompt: str, max_tokens: int = 1100, temperature: float = 0.4, retry_attempts: int = 3) -> str:
        """
        Generate content from the OpenAI API using the provided prompt and parameters (non-streaming).
        """
        logger.debug(f"Generating non-streaming payload with prompt: {prompt}")

        attempts = 0
        while attempts < retry_attempts:
            try:
                chat_completion = client.chat.completions.create(
                    messages=[{"role": "user", "content": prompt}],
                    model="gpt-4o",
                    max_tokens=max_tokens,
                    temperature=temperature
                )

                content = chat_completion.choices[0].message.content.strip()
                logger.debug(f"Generated payload: {content}")
                return content

            except Exception as e:
                attempts += 1
                logger.error(f"Error generating payload (attempt {attempts}): {str(e)}")
                if attempts >= retry_attempts:
                    raise Exception(f"Failed to generate payload after {retry_attempts} attempts") from e
                logger.info("Retrying to generate payload...")

    def generate_payload_stream(self, prompt: str, max_tokens: int = 1100, temperature: float = 0.4, retry_attempts: int = 3):
        """
        Generate content from the OpenAI API using the provided prompt and parameters, streaming the response.
        This returns a generator that yields partial text chunks as they arrive.
        """
        logger.debug(f"Generating streaming payload with prompt: {prompt}")

        try:
            response = client.chat.completions.create(
                messages=[{"role": "user", "content": prompt}],
                model="gpt-4o",
                max_tokens=max_tokens,
                temperature=temperature,
                stream=True  
            )


            for chunk in response:
                if chunk.choices:
                    delta = chunk.choices[0].delta
                    chunk_content = getattr(delta, "content", None)
                    if chunk_content:
                        yield chunk_content

        except Exception as e:
            logger.error(f"Error while streaming payload: {str(e)}")
            yield f"\n[Error occurred during streaming: {str(e)}]\n"

================
File: models/log_history.py
================
# models/log_history.py
from datetime import datetime
from pydantic import BaseModel, Field
from typing import Union
from uuid import UUID, uuid4

from .log_models import (
    Log,
    FirewallLog, VulnerabilityLog, IntrusionLog, AccessControlLog,
    SystemEvent, ApplicationEvent, AuthenticationEvent, NetworkEvent,
    DatabaseErrorLog, FileSystemErrorLog, NetworkErrorLog, ApplicationErrorLog,
    QueryDebugLog, ApiDebugLog, ConfigDebugLog, ProcessDebugLog,
    SystemInfoLog, UserActivityLog, DeploymentLog, ServiceStatusLog
)


LogType = Union[
    FirewallLog, VulnerabilityLog, IntrusionLog, AccessControlLog,
    SystemEvent, ApplicationEvent, AuthenticationEvent, NetworkEvent,
    DatabaseErrorLog, FileSystemErrorLog, NetworkErrorLog, ApplicationErrorLog,
    QueryDebugLog, ApiDebugLog, ConfigDebugLog, ProcessDebugLog,
    SystemInfoLog, UserActivityLog, DeploymentLog, ServiceStatusLog
]

class LogHistory(BaseModel):
    id: UUID = Field(default_factory=uuid4, description="Unique identifier for the log history entry")
    log: LogType = Field(..., description="The log entry associated with this history record")
    analysis: str = Field(..., description="Result or summary of the log analysis")
    timestamp: datetime = Field(default_factory=datetime.utcnow, description="Timestamp when the analysis was performed")

================
File: models/log_models.py
================
from pydantic import BaseModel, Field, validator
from typing import Optional, List, Dict
from datetime import datetime

class Log(BaseModel):
    type: str
    source: str
    event: str
    message: str
    severity: str = Field(..., description="Severity level")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
    metadata: Optional[Dict[str, str]] = None

    @validator('severity')
    def validate_severity(cls, v):
        allowed = {"critical", "high", "medium", "low", "info", "debug"}
        if v.lower() not in allowed:
            raise ValueError(f"Severity must be one of {allowed}")
        return v.lower()

class SecurityLog(Log):
    ip_address: Optional[str] = None
    username: Optional[str] = None
    attack_vector: Optional[str] = None
    status: Optional[str] = None
    geolocation: Optional[Dict[str, str]] = None
    device_info: Optional[Dict[str, str]] = None

class FirewallLog(SecurityLog):
    source_ip: str
    destination_ip: str
    action: str
    protocol: str
    port: int
    rule_triggered: str
    traffic_volume: str
    session_id: Optional[str] = None
    destination_country: Optional[str] = None
    source_country: Optional[str] = None

class VulnerabilityLog(SecurityLog):
    tool: str
    vulnerability_name: str
    cve_id: Optional[str] = None
    severity_level: str
    affected_components: List[str]
    remediation_steps: List[str]
    discovery_date: datetime
    patch_available: bool

class IntrusionLog(SecurityLog):
    detection_system: str
    intrusion_method: str
    risk_level: str
    source_ip: str
    destination_ip: str
    attempted_payloads: List[str]
    response_actions: List[str]
    breach_confirmed: bool
    compromised_data: Optional[List[str]] = None

class AccessControlLog(SecurityLog):
    access_type: str
    resource: str
    access_reason: str
    access_time: datetime
    access_location: str
    authentication_method: str
    user_agent: Optional[str] = None
    session_duration: Optional[str] = None

class EventLog(Log):
    system_component: str
    action_performed: str
    result: str
    additional_info: Optional[Dict[str, str]] = None
    user_id: Optional[str] = None
    related_processes: Optional[List[str]] = None
    event_type: Optional[str] = None

class SystemEvent(EventLog):
    os_version: str
    kernel_version: Optional[str] = None
    uptime: str
    resource_usage: Dict[str, str]
    network_interfaces: List[str]
    running_services: List[str]
    last_patch_applied: Optional[str] = None
    scheduled_maintenance: Optional[bool] = None
    hardware_health: Optional[Dict[str, str]] = None
    software_updates: Optional[List[str]] = None

class ApplicationEvent(EventLog):
    application_name: str
    version: str
    action_details: str
    error_code: Optional[str] = None
    stack_trace: Optional[str] = None
    deployment_version: Optional[str] = None
    rollback_occurred: Optional[bool] = None
    affected_users: Optional[List[str]] = None

class AuthenticationEvent(EventLog):
    auth_method: str
    auth_status: str
    user_role: Optional[str] = None
    session_id: Optional[str] = None
    device_info: Optional[str] = None
    login_attempts: Optional[int] = None
    lockout_status: Optional[bool] = None
    last_login_time: Optional[datetime] = None

class NetworkEvent(EventLog):
    src_ip: str
    dest_ip: str
    protocol: str
    action_taken: str
    bytes_transferred: str
    connection_duration: str
    traffic_type: str
    network_device: Optional[str] = None
    bandwidth_utilization: Optional[str] = None
    latency: Optional[str] = None

class ErrorLog(Log):
    error_code: str
    error_message: str
    module: Optional[str] = None
    error_details: Optional[str] = None
    stack_trace: Optional[str] = None
    user_impact: Optional[str] = None
    resolution_steps: Optional[List[str]] = None
    reported_by: Optional[str] = None

class DatabaseErrorLog(ErrorLog):
    query: str
    database_name: str
    db_engine: str
    affected_tables: List[str]
    transaction_id: Optional[str] = None
    affected_rows: Optional[int] = None
    replication_status: Optional[str] = None

class FileSystemErrorLog(ErrorLog):
    file_path: str
    file_operation: str
    error_details: str
    user_id: Optional[str] = None
    disk_space_remaining: Optional[str] = None
    file_size: Optional[str] = None
    file_type: Optional[str] = None

class NetworkErrorLog(ErrorLog):
    interface: str
    error_cause: str
    affected_service: str
    retry_attempts: int
    resolution_status: str
    packet_loss: Optional[str] = None
    throughput: Optional[str] = None

class ApplicationErrorLog(ErrorLog):
    app_name: str
    version: str
    crash_report: Optional[str] = None
    user_feedback: Optional[str] = None
    last_user_action: Optional[str] = None
    session_id: Optional[str] = None
    dependencies_affected: Optional[List[str]] = None
    logs_generated: Optional[List[str]] = None

class DebugLog(Log):
    debug_message: str
    module_name: str
    debug_level: str
    additional_data: Optional[Dict[str, str]] = None
    thread_id: Optional[str] = None
    function_name: Optional[str] = None
    execution_flow: Optional[List[str]] = None

class QueryDebugLog(DebugLog):
    sql_query: str
    execution_time: float
    records_returned: int
    database_connection: str
    query_plan: Optional[str] = None
    affected_rows: Optional[int] = None

class ApiDebugLog(DebugLog):
    api_endpoint: str
    http_method: str
    response_time: float
    status_code: int
    request_payload: Optional[Dict[str, str]] = None
    response_payload: Optional[Dict[str, str]] = None
    headers_sent: Optional[Dict[str, str]] = None
    headers_received: Optional[Dict[str, str]] = None

class ConfigDebugLog(DebugLog):
    config_file: str
    settings_applied: List[str]
    previous_settings: Optional[Dict[str, str]] = None
    update_method: str
    rollback_performed: Optional[bool] = None
    validation_results: Optional[Dict[str, str]] = None

class ProcessDebugLog(DebugLog):
    process_id: int
    process_name: str
    execution_status: str
    memory_usage: str
    cpu_usage: str
    parent_process_id: Optional[int] = None
    thread_ids: Optional[List[int]] = None
    open_files: Optional[List[str]] = None
    network_connections: Optional[List[str]] = None

class InfoLog(Log):
    summary: str
    details: str
    source_system: Optional[str] = None
    related_events: Optional[List[str]] = None
    user_id: Optional[str] = None
    affected_components: Optional[List[str]] = None
    additional_context: Optional[Dict[str, str]] = None

class SystemInfoLog(InfoLog):
    os_version: str
    uptime: str
    resource_usage: Dict[str, str]
    network_interfaces: List[str]
    running_services: List[str]
    last_patch_applied: Optional[str] = None
    scheduled_maintenance: Optional[bool] = None
    hardware_health: Optional[Dict[str, str]] = None
    software_updates: Optional[List[str]] = None

class UserActivityLog(InfoLog):
    user_id: str
    activity_type: str
    activity_details: str
    location: str
    device_used: str
    login_time: Optional[datetime] = None
    logout_time: Optional[datetime] = None
    actions_performed: Optional[List[str]] = None
    session_duration: Optional[str] = None

class DeploymentLog(InfoLog):
    environment: str
    deployment_status: str
    release_version: str
    deployment_time: datetime = Field(default_factory=datetime.utcnow)
    deployed_by: str
    rollback_needed: bool
    deployment_notes: Optional[str] = None
    impacted_services: Optional[List[str]] = None
    pre_deployment_checks: Optional[Dict[str, str]] = None

class ServiceStatusLog(InfoLog):
    service_name: str
    status: str
    last_checked: datetime = Field(default_factory=datetime.utcnow)
    response_time: float
    uptime_percentage: float
    error_count: int
    dependent_services: Optional[List[str]] = None
    maintenance_window: Optional[Dict[str, str]] = None
    health_metrics: Optional[Dict[str, str]] = None

class LogResponse(BaseModel):
    logs: List[Log]
    count: int

================
File: models/newsletter_content.py
================
# models/newsletter_content.py

from pymongo import MongoClient
import os
from dotenv import load_dotenv

load_dotenv()

mongo_uri = os.getenv("MONGO_URI")
client = MongoClient(mongo_uri)
db = client.get_database()

newsletter_collection = db["newsletter"] 

def get_current_newsletter_db():
    """
    Returns the single doc that stores the current newsletter content.
    """
    return newsletter_collection.find_one({})

def set_current_newsletter_db(content):
    """
    Overwrite the single doc with new content.
    """
 
    newsletter_collection.delete_many({})

    newsletter_collection.insert_one({"content": content})

================
File: models/test.py
================
from bson.objectid import ObjectId
from datetime import datetime, timedelta
from collections import defaultdict
import math
import re
import unicodedata

# Import the new collections from database
from mongodb.database import (
    mainusers_collection,
    shop_collection,
    achievements_collection,
    tests_collection,
    testAttempts_collection,
    correctAnswers_collection
)

##############################################
# very complex Input Sanitization Helpers
##############################################

import re
import unicodedata

# Example small dictionary of very common passwords
COMMON_PASSWORDS = {
    "password", "123456", "12345678", "qwerty", "letmein", "welcome"
}

def has_forbidden_unicode_scripts(s):
    """
    Disallow characters from certain Unicode blocks 
    (private use areas, surrogates, etc.).
    """
    private_use_ranges = [
        (0xE000, 0xF8FF),
        (0xF0000, 0xFFFFD),
        (0x100000, 0x10FFFD)
    ]
    surrogates_range = (0xD800, 0xDFFF)

    for ch in s:
        code_point = ord(ch)
        # Surrogates
        if surrogates_range[0] <= code_point <= surrogates_range[1]:
            return True
        # Private use ranges
        for start, end in private_use_ranges:
            if start <= code_point <= end:
                return True
    return False

def disallow_mixed_scripts(s):
    """
    Example check for mixing major scripts (Latin + Cyrillic, etc.).
    Returns True if it detects more than one script in the string.
    """
    script_sets = set()

    for ch in s:
        cp = ord(ch)
        # Basic Latin and extended ranges:
        if 0x0041 <= cp <= 0x024F:
            script_sets.add("Latin")
        # Greek
        elif 0x0370 <= cp <= 0x03FF:
            script_sets.add("Greek")
        # Cyrillic
        elif 0x0400 <= cp <= 0x04FF:
            script_sets.add("Cyrillic")

        # If more than one distinct script is found
        if len(script_sets) > 1:
            return True

    return False

def validate_username(username):
    """
    Validates a username with very strict rules:
      1. Normalize (NFC).
      2. Length 3..30.
      3. No control chars, no private-use/surrogates, no mixing scripts.
      4. Only [A-Za-z0-9._-], no triple repeats, no leading/trailing punctuation.
    Returns: (True, []) if valid, else (False, [list of error messages]).
    """
    errors = []
    username_nfc = unicodedata.normalize("NFC", username)

    # 1) Check length
    if not (3 <= len(username_nfc) <= 30):
        errors.append("Username must be between 3 and 30 characters long.")

    # 2) Forbidden Unicode script checks
    if has_forbidden_unicode_scripts(username_nfc):
        errors.append("Username contains forbidden Unicode blocks (private use or surrogates).")

    # 3) Disallow mixing multiple major scripts
    if disallow_mixed_scripts(username_nfc):
        errors.append("Username cannot mix multiple Unicode scripts (e.g., Latin & Cyrillic).")

    # 4) Forbid control chars [0..31, 127] + suspicious punctuation
    forbidden_ranges = [(0, 31), (127, 127)]
    forbidden_chars = set(['<', '>', '\\', '/', '"', "'", ';', '`',
                           ' ', '\t', '\r', '\n'])
    for ch in username_nfc:
        cp = ord(ch)
        if any(start <= cp <= end for (start, end) in forbidden_ranges):
            errors.append("Username contains forbidden control characters (ASCII 0-31 or 127).")
            break
        if ch in forbidden_chars:
            errors.append("Username contains forbidden characters like <, >, or whitespace.")
            break

    # 5) Strict allowlist pattern
    pattern = r'^[A-Za-z0-9._-]+$'
    if not re.match(pattern, username_nfc):
        errors.append("Username can only contain letters, digits, underscores, dashes, or dots.")

    # 6) Disallow triple identical consecutive characters
    if re.search(r'(.)\1{2,}', username_nfc):
        errors.append("Username cannot contain three identical consecutive characters.")

    # 7) Disallow leading or trailing punctuation
    if re.match(r'^[._-]|[._-]$', username_nfc):
        errors.append("Username cannot start or end with . - or _.")

    if errors:
        return False, errors
    return True, []

def validate_password(password, username=None, email=None):
    """
    Validates a password with very strict rules:
      1. 12..128 length.
      2. Disallow whitespace, <, >.
      3. Require uppercase, lowercase, digit, special char.
      4. Disallow triple repeats.
      5. Check common/breached password list.
      6. Disallow 'password', 'qwerty', etc.
      7. Disallow if username or email local part is in the password.
    Returns: (True, []) if valid, else (False, [list of error messages]).
    """
    errors = []
    length = len(password)

    # 1) Length
    if not (6 <= length <= 69):
        errors.append("Password must be between 6 and 69 characters long.")

    # 2) Disallowed whitespace or < >
    if any(ch in password for ch in [' ', '<', '>', '\t', '\r', '\n']):
        errors.append("Password cannot contain whitespace or < or > characters.")

    # 3) Complexity checks
    if not re.search(r'[A-Z]', password):
        errors.append("Password must contain at least one uppercase letter.")
    if not re.search(r'[a-z]', password):
        errors.append("Password must contain at least one lowercase letter.")
    if not re.search(r'\d', password):
        errors.append("Password must contain at least one digit.")

    # We define a broad set of allowed special chars
    special_pattern = r'[!@#$%^&*()\-_=+\[\]{}|;:\'",<.>/?`~\\]'
    if not re.search(special_pattern, password):
        errors.append("Password must contain at least one special character.")

    # 4) Disallow triple identical consecutive characters
    if re.search(r'(.)\1{2,}', password):
        errors.append("Password must not contain three identical consecutive characters.")

    # 5) Convert to lowercase for simplified checks
    password_lower = password.lower()

    # Check against common password list
    if password_lower in COMMON_PASSWORDS:
        errors.append("Password is too common. Please choose a stronger password.")

    # 6) Disallow certain dictionary words
    dictionary_patterns = ['password', 'qwerty', 'abcdef', 'letmein', 'welcome', 'admin']
    for pat in dictionary_patterns:
        if pat in password_lower:
            errors.append(f"Password must not contain the word '{pat}'.")

    # 7) Disallow if password contains username or email local-part
    if username:
        if username.lower() in password_lower:
            errors.append("Password must not contain your username.")

    if email:
        email_local_part = email.split('@')[0].lower()
        if email_local_part in password_lower:
            errors.append("Password must not contain the local part of your email address.")

    if errors:
        return False, errors
    return True, []

def validate_email(email):
    """
    Validates an email with strict rules:
      1. Normalize (NFC), strip whitespace.
      2. 5..69 length.
      3. No control chars, <, >, etc.
      4. Exactly one @.
    Returns: (True, []) if valid, else (False, [list of error messages]).
    """
    errors = []
    email_nfc = unicodedata.normalize("NFC", email.strip())

    # 1) Length check
    if not (5 <= len(email_nfc) <= 69):
        errors.append("Email length must be between 6 and 69 characters.")

    # 3) Forbid suspicious ASCII
    forbidden_ascii = set(['<','>','`',';',' ', '\t','\r','\n','"',"'", '\\'])
    for ch in email_nfc:
        if ch in forbidden_ascii:
            errors.append("Email contains forbidden characters like <, >, or whitespace.")
            break

    # 4) Must have exactly one @
    if email_nfc.count('@') != 1:
        errors.append("Email must contain exactly one '@' symbol.")

    if errors:
        return False, errors
    return True, []

##############################################
# User Retrieval Helpers
##############################################

def get_user_by_username(username):
    return mainusers_collection.find_one({"username": username})

def get_user_by_identifier(identifier):
    if "@" in identifier:
        return mainusers_collection.find_one({"email": identifier})
    else:
        return get_user_by_username(identifier)

def get_user_by_id(user_id):
    """
    Retrieves a user by ID. Returns None if invalid or not found.
    """
    try:
        oid = ObjectId(user_id)
    except Exception:
        return None
    return mainusers_collection.find_one({"_id": oid})

##############################################
# Create User
##############################################

def create_user(user_data):
    """
    Creates a new user document, setting default fields including coins, xp, level,
    purchasedItems, xpBoost, etc. Also automatically equips a default avatar if found.
    """
    existing_user = mainusers_collection.find_one({
        "$or": [
            {"username": user_data["username"]},
            {"email": user_data["email"]}
        ]
    })
    if existing_user:
        raise ValueError("Username or email is already taken")

    # Set defaults for new user:
    user_data.setdefault("coins", 0)
    user_data.setdefault("xp", 0)
    user_data.setdefault("level", 1)
    user_data.setdefault("achievements", [])
    user_data.setdefault("subscriptionActive", False)
    user_data.setdefault("subscriptionPlan", None)
    user_data.setdefault("lastDailyClaim", None)
    user_data.setdefault("purchasedItems", [])
    user_data.setdefault("xpBoost", 1.0)
    user_data.setdefault("currentAvatar", None)
    user_data.setdefault("nameColor", None)

    # Auto-equip default avatar if cost=null
    default_avatar = shop_collection.find_one({"type": "avatar", "cost": None})
    if default_avatar:
        user_data["currentAvatar"] = default_avatar["_id"]
        if default_avatar["_id"] not in user_data["purchasedItems"]:
            user_data["purchasedItems"].append(default_avatar["_id"])

    result = mainusers_collection.insert_one(user_data)
    return result.inserted_id

##############################################
# Update User Fields (CRITICAL)
##############################################

def update_user_fields(user_id, fields):
    """
    Generic helper to update given `fields` (dict) in mainusers_collection.
    """
    try:
        oid = ObjectId(user_id)
    except:
        return None
    mainusers_collection.update_one(
        {"_id": oid},
        {"$set": fields}
    )
    return True

##############################################
# Update User Coins
##############################################

def update_user_coins(user_id, amount):
    try:
        oid = ObjectId(user_id)
    except Exception:
        return None
    mainusers_collection.update_one({"_id": oid}, {"$inc": {"coins": amount}})

##############################################
# Leveling System
##############################################
# Levels 2–30: +500 XP each
# Levels 31–60: +750 XP each
# Levels 61–100: +1000 XP each
# Above 100: +1500 XP each

def xp_required_for_level(level):
    """
    Returns total XP required to be at `level`.
    Level 1 starts at 0 XP.
    """
    if level < 1:
        return 0
    if level == 1:
        return 0
    if level <= 30:
        return 500 * (level - 1)
    elif level <= 60:
        base = 500 * 29  # up to level 30
        return base + 750 * (level - 30)
    elif level <= 100:
        base = 500 * 29 + 750 * 30  # up to level 60
        return base + 1000 * (level - 60)
    else:
        base = 500 * 29 + 750 * 30 + 1000 * 40  # up to level 100
        return base + 1500 * (level - 100)

def update_user_xp(user_id, xp_to_add):
    """
    Adds xp_to_add to the user's XP. Then, while the new XP total
    is >= XP required for the next level, increments the level.
    """
    user = get_user_by_id(user_id)
    if not user:
        return None

    old_xp = user.get("xp", 0)
    old_level = user.get("level", 1)
    new_xp = old_xp + xp_to_add
    new_level = old_level

    while new_xp >= xp_required_for_level(new_level + 1):
        new_level += 1

    mainusers_collection.update_one(
        {"_id": user["_id"]},
        {"$set": {"xp": new_xp, "level": new_level}}
    )
    return {"xp": new_xp, "level": new_level}

##############################################
# Daily Bonus
##############################################

def apply_daily_bonus(user_id):
    """
    If the user hasn't claimed daily bonus in the last 24 hours,
    +50 coins, update lastDailyClaim
    """
    user = get_user_by_id(user_id)
    if not user:
        return None

    now = datetime.utcnow()
    last_claimed = user.get("lastDailyClaim")
    if not last_claimed or (now - last_claimed) > timedelta(hours=24):
        mainusers_collection.update_one(
            {"_id": user["_id"]},
            {"$inc": {"coins": 50}, "$set": {"lastDailyClaim": now}}
        )
        return {"success": True, "message": "Daily bonus applied"}
    else:
        return {"success": False, "message": "Already claimed daily bonus"}

##############################################
# Shop Logic
##############################################

def get_shop_items():
    """
    Returns all shop items from shop_collection,
    in ascending order by title (or another field),
    to ensure stable ordering.
    """
    return list(shop_collection.find({}).sort("title", 1))

def purchase_item(user_id, item_id):
    """
    Purchase an item from the shop:
      1) Check user has enough coins
      2) Ensure item not already purchased
      3) Deduct cost, add to purchasedItems
      4) If xpBoost, set user's xpBoost
      5) If avatar or nameColor, optionally set that field
    """
    user = get_user_by_id(user_id)
    if not user:
        return {"success": False, "message": "User not found"}

    try:
        oid = ObjectId(item_id)
    except Exception:
        return {"success": False, "message": "Invalid item ID"}

    item = shop_collection.find_one({"_id": oid})
    if not item:
        return {"success": False, "message": "Item not found"}

    user_coins = user.get("coins", 0)
    cost = item.get("cost", 0) if item.get("cost") is not None else 0
    if user_coins < cost:
        return {"success": False, "message": "Not enough coins"}

    purchased = user.get("purchasedItems", [])
    if oid in purchased:
        return {"success": False, "message": "Item already purchased"}

    mainusers_collection.update_one(
        {"_id": user["_id"]},
        {"$inc": {"coins": -cost}}
    )
    mainusers_collection.update_one(
        {"_id": user["_id"]},
        {"$addToSet": {"purchasedItems": oid}}
    )

    item_type = item.get("type")
    if item_type == "xpBoost":
        new_boost = item.get("effectValue", 1.0)
        mainusers_collection.update_one(
            {"_id": user["_id"]},
            {"$set": {"xpBoost": new_boost}}
        )
    elif item_type == "avatar":
        pass
    elif item_type == "nameColor":
        new_color = item.get("effectValue", None)
        mainusers_collection.update_one(
            {"_id": user["_id"]},
            {"$set": {"nameColor": new_color}}
        )

    return {"success": True, "message": "Purchase successful"}

##############################################
# Achievements
##############################################

def get_achievements():
    return list(achievements_collection.find({}))

def get_test_by_id_and_category(test_id, category):
    """
    Fetch a single test doc by integer testId field and category field.
    """
    try:
        test_id_int = int(test_id)
    except:
        return None
    return tests_collection.find_one({
        "testId": test_id_int,
        "category": category
    })

def check_and_unlock_achievements(user_id):
    """
    Checks the user's progress by querying testAttempts_collection to see:
      - How many tests are finished (total_finished)
      - How many are perfect (perfect_tests)
      - Their percentage on each finished test
      - If they've done certain minScores, consecutive perfects, etc.
      - Summation of total questions answered across all finished attempts

    Then unlocks achievements as needed, returning newly_unlocked achievementIds.
    """

    user = get_user_by_id(user_id)
    if not user:
        return []

    user_oid = user["_id"]

    # 1) Count how many finished attempts the user has
    total_finished = testAttempts_collection.count_documents({
        "userId": user_oid,
        "finished": True
    })

    # 2) Count how many are perfect (score == totalQuestions)
    perfect_tests = testAttempts_collection.count_documents({
        "userId": user_oid,
        "finished": True,
        "$expr": {"$eq": ["$score", "$totalQuestions"]}
    })

    # 3) Fetch all finished attempts
    finished_cursor = testAttempts_collection.find(
        {"userId": user_oid, "finished": True}
    )
    finished_tests = []
    for doc in finished_cursor:
        tq = doc.get("totalQuestions", 0)
        sc = doc.get("score", 0)
        pct = (sc / tq) * 100 if tq else 0
        cat = doc.get("category", "global")
        finished_at = doc.get("finishedAt", None)
        finished_tests.append({
            "test_id": doc.get("testId", "0"),
            "score": sc,
            "totalQuestions": tq,
            "percentage": pct,
            "category": cat,
            "finishedAt": finished_at
        })

    from datetime import datetime
    finished_tests.sort(
        key=lambda x: x["finishedAt"] if x["finishedAt"] else datetime(1970,1,1)
    )

    max_consecutive = 0
    current_streak = 0
    for ft in finished_tests:
        if ft["percentage"] == 100:
            current_streak += 1
            if current_streak > max_consecutive:
                max_consecutive = current_streak
        else:
            current_streak = 0

    from collections import defaultdict
    category_groups = defaultdict(list)
    for ft in finished_tests:
        category_groups[ft["category"]].append(ft)

    sum_of_questions = sum(ft["totalQuestions"] for ft in finished_tests)

    TOTAL_TESTS = 130
    TOTAL_QUESTIONS = 10000

    user_coins = user.get("coins", 0)
    user_level = user.get("level", 1)

    unlocked = user.get("achievements", [])
    newly_unlocked = []

    all_ach = get_achievements()

    for ach in all_ach:
        aid = ach["achievementId"]
        criteria = ach.get("criteria", {})

        if aid in unlocked:
            continue

        # testCount
        if "testCount" in criteria:
            if total_finished >= criteria["testCount"]:
                unlocked.append(aid)
                newly_unlocked.append(aid)

        # coins
        if "coins" in criteria:
            if user_coins >= criteria["coins"]:
                unlocked.append(aid)
                newly_unlocked.append(aid)

        # level
        if "level" in criteria:
            if user_level >= criteria["level"]:
                unlocked.append(aid)
                newly_unlocked.append(aid)

        # perfectTests
        if "perfectTests" in criteria:
            needed = criteria["perfectTests"]
            if perfect_tests >= needed:
                unlocked.append(aid)
                newly_unlocked.append(aid)

        # consecutivePerfects
        if "consecutivePerfects" in criteria:
            needed = criteria["consecutivePerfects"]
            if max_consecutive >= needed:
                unlocked.append(aid)
                newly_unlocked.append(aid)

        # allTestsCompleted
        if "allTestsCompleted" in criteria and criteria["allTestsCompleted"] is True:
            if total_finished >= TOTAL_TESTS:
                unlocked.append(aid)
                newly_unlocked.append(aid)

        # testsCompletedInCategory
        if "testsCompletedInCategory" in criteria:
            needed = criteria["testsCompletedInCategory"]
            for ccat, attempts in category_groups.items():
                if len(attempts) >= needed:
                    unlocked.append(aid)
                    newly_unlocked.append(aid)
                    break

        # redemption_arc => minScoreBefore & minScoreAfter
        if ("minScoreBefore" in criteria and "minScoreAfter" in criteria
                and aid not in unlocked):
            min_before = criteria["minScoreBefore"]
            min_after = criteria["minScoreAfter"]
            low_test = any(ft["percentage"] <= min_before for ft in finished_tests)
            high_test = any(ft["percentage"] >= min_after for ft in finished_tests)
            if low_test and high_test:
                unlocked.append(aid)
                newly_unlocked.append(aid)

        # minScore => e.g. "accuracy_king"
        if "minScore" in criteria:
            needed = criteria["minScore"]
            if any(ft["percentage"] >= needed for ft in finished_tests):
                unlocked.append(aid)
                newly_unlocked.append(aid)

        # minScoreGlobal => e.g. "exam_conqueror"
        if "minScoreGlobal" in criteria:
            min_g = criteria["minScoreGlobal"]
            if total_finished >= TOTAL_TESTS:
                all_above = all(ft["percentage"] >= min_g for ft in finished_tests)
                if all_above:
                    unlocked.append(aid)
                    newly_unlocked.append(aid)

        # minScoreInCategory => e.g. "subject_specialist"
        if "minScoreInCategory" in criteria:
            min_cat = criteria["minScoreInCategory"]
            for ccat, attempts in category_groups.items():
                if len(attempts) == 10:
                    if all(ft["percentage"] >= min_cat for ft in attempts):
                        unlocked.append(aid)
                        newly_unlocked.append(aid)
                        break

        # perfectTestsInCategory => "category_perfectionist"
        if "perfectTestsInCategory" in criteria:
            needed = criteria["perfectTestsInCategory"]
            for ccat, attempts in category_groups.items():
                perfect_count = sum(1 for ft in attempts if ft["percentage"] == 100)
                if perfect_count >= needed:
                    unlocked.append(aid)
                    newly_unlocked.append(aid)
                    break

        # perfectTestsGlobal => "absolute_perfectionist"
        if "perfectTestsGlobal" in criteria and criteria["perfectTestsGlobal"] is True:
            if total_finished >= TOTAL_TESTS:
                all_perfect = all(ft["percentage"] == 100 for ft in finished_tests)
                if all_perfect:
                    unlocked.append(aid)
                    newly_unlocked.append(aid)

        # totalQuestions => e.g. "answer_machine_1000"
        if "totalQuestions" in criteria:
            needed_q = criteria["totalQuestions"]
            if sum_of_questions >= needed_q:
                unlocked.append(aid)
                newly_unlocked.append(aid)

    if newly_unlocked:
        mainusers_collection.update_one(
            {"_id": user_oid},
            {"$set": {"achievements": unlocked}}
        )

    return newly_unlocked

================
File: models/user_subscription.py
================
# models/user_subscription.py

from pymongo import MongoClient
import os
from dotenv import load_dotenv
import logging

load_dotenv()

mongo_uri = os.getenv("MONGO_URI")
client = MongoClient(mongo_uri)
db = client.get_database()

users_collection = db["users"]


logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

def create_user(user_data):
    """
    Create a new user in the users collection.
    :param user_data: Dictionary containing user details (e.g., {"email": "alice@example.com"})
    :return: The ID of the created or existing user
    """
    email = user_data.get("email")
    if not email:
        logger.error("Email is required to create a user.")
        raise ValueError("Email is required to create a user.")

    existing_user = users_collection.find_one({"email": email})
    if existing_user:
        logger.info(f"User with email {email} already exists with ID {existing_user['_id']}.")
        return existing_user["_id"]

    result = users_collection.insert_one({"email": email})
    logger.info(f"Created new user with email {email} and ID {result.inserted_id}.")
    return result.inserted_id

def add_subscription(email: str):
    """
    Add a user subscription if it doesn't already exist.
    :param email: User's email address
    """
    existing = users_collection.find_one({"email": email})
    if not existing:
        users_collection.insert_one({"email": email})
        logger.info(f"Added new subscription for email: {email}")
    else:
        logger.info(f"Subscription already exists for email: {email}")

def remove_subscription(email: str):
    """
    Remove a user subscription by email.
    :param email: User's email address
    """
    result = users_collection.delete_one({"email": email})
    if result.deleted_count > 0:
        logger.info(f"Removed subscription for email: {email}")
    else:
        logger.warning(f"No subscription found for email: {email}")

def find_subscription(email: str):
    """
    Find a user subscription by email.
    :param email: User's email address
    :return: The subscription document if found, else None
    """
    return users_collection.find_one({"email": email})

def get_all_subscribers():
    """
    Retrieve all user subscriptions.
    :return: List of all subscription documents
    """
    return list(users_collection.find({}))

================
File: mongodb/database.py
================
# database.py
from flask import Flask
from flask_pymongo import PyMongo
from dotenv import load_dotenv
import os

load_dotenv()

app = Flask(__name__)

# MongoDB Connection
app.config["MONGO_URI"] = os.getenv("MONGO_URI")  
mongo = PyMongo(app)

db = mongo.db

# Existing collections
mainusers_collection = db.mainusers
shop_collection = db.shopItems
achievements_collection = db.achievements
tests_collection = db.tests

# NEW collections for attempts and correct answers:
testAttempts_collection = db.testAttempts
correctAnswers_collection = db.correctAnswers

================
File: routes/admin_newsletter_routes.py
================
# backend/routes/admin_newsletter_routes.py

from flask import Blueprint, request, jsonify
from helpers.daily_newsletter_helper import set_current_newsletter_db
from functools import wraps
import os
from dotenv import load_dotenv

load_dotenv()

admin_newsletter_bp = Blueprint('admin_newsletter_routes', __name__)

def require_api_key(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        api_key = request.headers.get('x-api-key')
        if not api_key or api_key != os.getenv("ADMIN_API_KEY"):
            return jsonify({"error": "Unauthorized"}), 401
        return f(*args, **kwargs)
    return decorated

@admin_newsletter_bp.route('/', methods=['POST'])
@require_api_key
def update_newsletter():
    """
    POST /admin/newsletter
    Headers: { "x-api-key": "your_admin_api_key" }
    JSON Body: { "content": "<html or text of new newsletter>" }
    """
    try:
        data = request.get_json()
        new_content = data.get("content", "")

        if not new_content:
            return jsonify({"error": "Newsletter content cannot be empty."}), 400

        set_current_newsletter_db(new_content)
        return jsonify({"message": "Newsletter updated successfully"}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

================
File: routes/analogy_routes.py
================
from flask import Blueprint, request, jsonify, Response
import logging
from helpers.async_tasks import (
    generate_single_analogy_task,
    generate_comparison_analogy_task,
    generate_triple_comparison_analogy_task
)
# New streaming helper
from helpers.analogy_stream_helper import generate_analogy_stream

analogy_bp = Blueprint('analogy_bp', __name__)
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

@analogy_bp.route('/generate_analogy', methods=['POST'])
def generate_analogy():
    """
    OLD route that uses Celery tasks. We keep it so async_tasks or older code won't break,
    but the new front end won't use this route anymore.
    """
    data = request.get_json()
    if not data:
        return jsonify({"error": "Request must contain data"}), 400

    analogy_type = data.get("analogy_type")
    category = data.get("category")
    concept1 = data.get("concept1")
    concept2 = data.get("concept2")
    concept3 = data.get("concept3")

    try:
        if analogy_type == "single" and concept1:
            async_result = generate_single_analogy_task.delay(concept1, category)
            analogy_text = async_result.get(timeout=120)
            return jsonify({"analogy": analogy_text}), 200

        elif analogy_type == "comparison" and concept1 and concept2:
            async_result = generate_comparison_analogy_task.delay(concept1, concept2, category)
            analogy_text = async_result.get(timeout=120)
            return jsonify({"analogy": analogy_text}), 200

        elif analogy_type == "triple" and concept1 and concept2 and concept3:
            async_result = generate_triple_comparison_analogy_task.delay(concept1, concept2, concept3, category)
            analogy_text = async_result.get(timeout=180)
            return jsonify({"analogy": analogy_text}), 200

        else:
            logger.error("Invalid parameters provided to /generate_analogy")
            return jsonify({"error": "Invalid parameters"}), 400

    except Exception as e:
        logger.error(f"Error generating analogy (Celery route): {e}")
        return jsonify({"error": "An internal error occurred while generating the analogy."}), 500


@analogy_bp.route('/stream_analogy', methods=['POST'])
def stream_analogy():
    """
    NEW route that streams analogy text. Only used by front-end now.
    """
    data = request.get_json() or {}
    analogy_type = data.get("analogy_type", "single")
    category = data.get("category", "real-world")
    concept1 = data.get("concept1", "")
    concept2 = data.get("concept2", "")
    concept3 = data.get("concept3", "")

    try:
        def generate():
            stream_gen = generate_analogy_stream(analogy_type, concept1, concept2, concept3, category)
            for chunk in stream_gen:
                yield chunk

        return Response(generate(), mimetype='text/plain')

    except Exception as e:
        logger.error(f"Error streaming analogy: {e}")
        return jsonify({"error": "An internal error occurred while streaming the analogy."}), 500

================
File: routes/celery_routes.py
================
from flask import Blueprint, request, jsonify
from functools import wraps
import os

from helpers.daily_newsletter_task import send_daily_newsletter

celery_bp = Blueprint('celery_routes', __name__)

def require_api_key(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        api_key = request.headers.get('x-api-key')
        if not api_key or api_key != os.getenv("ADMIN_API_KEY"):
            return jsonify({"error": "Unauthorized"}), 401
        return f(*args, **kwargs)
    return decorated

@celery_bp.route('/trigger-task', methods=['POST'])
@require_api_key
def trigger_task():
    """
    POST /celery/trigger-task
    Headers: { "x-api-key": "<ADMIN_API_KEY>" }
    """
    try:
        task_result = send_daily_newsletter.delay()
        return jsonify({"message": "Task triggered", "task_id": task_result.id}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500

================
File: routes/daily_brief_routes.py
================
from flask import Blueprint, request, jsonify
from models.user_subscription import UserSubscription
from helpers.scheduler_helper import schedule_email_task

daily_brief_routes = Blueprint('daily_brief_routes', __name__)

@daily_brief_routes.route('/dailybrief/subscribe', methods=['POST'])
def subscribe():
    data = request.get_json()
    email = data.get('email')
    topic = data.get('topic')
    frequency = data.get('frequency')
    time_slots = data.get('time_slots')


    subscription = UserSubscription(email=email, topic=topic, frequency=frequency, time_slots=time_slots)
    subscription.save_to_db()

 
    for time_slot in time_slots:
        schedule_email_task(subscription, time_slot)

    return jsonify({"message": "Subscription created successfully"}), 201

================
File: routes/grc_routes.py
================
# grc_routes.py

from flask import Blueprint, request, jsonify
import logging
from helpers.async_tasks import generate_grc_question_task

grc_bp = Blueprint('grc', __name__)
logger = logging.getLogger(__name__)

GRC_CATEGORIES = ["Regulation", "Risk Management", "Compliance", "Audit", "Governance", 
                  "Management", "Policy", "Ethics", "Threat Assessment", "Leadership", 
                  "Business Continuity", "Random"]
DIFFICULTY_LEVELS = ["Easy", "Medium", "Hard"]

@grc_bp.route('/generate_question', methods=['POST'])
def generate_question():
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "Request must contain JSON data"}), 400

        category = data.get('category', 'Random')
        difficulty = data.get('difficulty', 'Easy')

        if category not in GRC_CATEGORIES:
            return jsonify({"error": "Invalid category"}), 400
        if difficulty not in DIFFICULTY_LEVELS:
            return jsonify({"error": "Invalid difficulty"}), 400

        # Celery call
        task_result = generate_grc_question_task.delay(category, difficulty)
        question_data = task_result.get(timeout=120)

        return jsonify(question_data), 200

    except Exception as e:
        logger.error(f"Error in /generate_question: {e}")
        return jsonify({"error": "An internal error occurred."}), 500

================
File: routes/log_routes.py
================
# log_routes.py

from flask import Blueprint, request, jsonify
from helpers.log_generator import generate_logs
from helpers.log_helper import analyze_log, serialize_log  
import logging
from models.log_models import (
    Log, SecurityLog, FirewallLog, VulnerabilityLog, IntrusionLog, AccessControlLog,
    EventLog, SystemEvent, ApplicationEvent, AuthenticationEvent, NetworkEvent,
    ErrorLog, DatabaseErrorLog, FileSystemErrorLog, NetworkErrorLog, ApplicationErrorLog,
    DebugLog, QueryDebugLog, ApiDebugLog, ConfigDebugLog, ProcessDebugLog,
    InfoLog, SystemInfoLog, UserActivityLog, DeploymentLog, ServiceStatusLog
)
from datetime import datetime


log_bp = Blueprint("logs", __name__)
logger = logging.getLogger(__name__)


if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.DEBUG)


@log_bp.route("/generate", methods=["POST"])
def generate_logs_route():
    """
    Endpoint to generate logs based on category and count.
    """
    try:
       
        data = request.get_json()
        category = data.get("category", "security").lower()
        count = int(data.get("count", 5))
        
        
        logs = generate_logs(category, count)
        if not logs:
            logger.warning(f"No logs generated for category: {category}")
            return jsonify({"error": "Invalid log category."}), 400
        
        
        serialized_logs = [serialize_log(log) for log in logs]

        logger.info(f"Generated {count} logs for category: {category}")
        return jsonify({
            "status": "success",
            "category": category,
            "logs": serialized_logs,
            "count": len(serialized_logs),
        }), 200

    except Exception as e:
        logger.error(f"Error generating logs: {e}")
        return jsonify({"error": str(e)}), 500



@log_bp.route("/analyze", methods=["POST"])
def analyze_log_route():
    """
    Endpoint to analyze a manually provided single log.
    """
    try:
        
        data = request.get_json()
        log = data.get("log", None)
        
        if not log:
            logger.warning("No log provided for analysis.")
            return jsonify({"error": "No log provided."}), 400
        
        # Deserializing logs back into model object
        log_type = log.get("type")
        source = log.get("source", "").lower()
        log_obj = None

        if log_type == "security":
            if source == "firewall":
                log_obj = FirewallLog(**log)
            elif source == "vulnerability scanner":
                log_obj = VulnerabilityLog(**log)
            elif source == "ids":
                log_obj = IntrusionLog(**log)
            elif source == "access control":
                log_obj = AccessControlLog(**log)
            else:
                logger.warning(f"Unknown security log source: {source}")
        elif log_type == "event":
            if source == "system monitor":
                log_obj = SystemEvent(**log)
            elif source == "appmanager":
                log_obj = ApplicationEvent(**log)
            elif source == "authservice":
                log_obj = AuthenticationEvent(**log)
            elif source == "network manager":
                log_obj = NetworkEvent(**log)
            else:
                logger.warning(f"Unknown event log source: {source}")
        elif log_type == "error":
            if source == "database":
                log_obj = DatabaseErrorLog(**log)
            elif source == "filesystem":
                log_obj = FileSystemErrorLog(**log)
            elif source == "network interface":
                log_obj = NetworkErrorLog(**log)
            elif source == "application service":
                log_obj = ApplicationErrorLog(**log)
            else:
                logger.warning(f"Unknown error log source: {source}")
        elif log_type == "debug":
            if source == "query executor":
                log_obj = QueryDebugLog(**log)
            elif source == "api gateway":
                log_obj = ApiDebugLog(**log)
            elif source == "config manager":
                log_obj = ConfigDebugLog(**log)
            elif source == "process manager":
                log_obj = ProcessDebugLog(**log)
            else:
                logger.warning(f"Unknown debug log source: {source}")
        elif log_type == "info":
            if source == "system monitor":
                log_obj = SystemInfoLog(**log)
            elif source == "activity tracker":
                log_obj = UserActivityLog(**log)
            elif source == "deployment manager":
                log_obj = DeploymentLog(**log)
            elif source == "service monitor":
                log_obj = ServiceStatusLog(**log)
            else:
                logger.warning(f"Unknown info log source: {source}")
        else:
            logger.warning(f"Unknown log type: {log_type}")
        
        if not log_obj:
            logger.warning("Invalid log data provided.")
            return jsonify({"error": "Invalid log data provided."}), 400

       
        analysis = analyze_log(log_obj)

       
        serialized_log = serialize_log(log_obj)

        logger.info(f"Analyzed log: {serialized_log.get('id', 'N/A')}")
        return jsonify({
            "status": "success",
            "log": serialized_log,
            "analysis": analysis
        }), 200

    except Exception as e:
        logger.error(f"Error analyzing log: {e}")
        return jsonify({"error": str(e)}), 500



@log_bp.route("/healthcheck", methods=["GET"])
def health_check():
    """
    Health check endpoint to ensure the service is running.
    """
    return jsonify({"status": "running", "service": "Log Analysis API"}), 200



def init_log_routes(db):
    """
    Initialize routes with database integration for log history.
    """
    @log_bp.route("/history/save", methods=["POST"])
    def save_log_history():
        try:
            data = request.get_json()
            db.log_history.insert_one(data)
            return jsonify({"status": "success", "message": "Log history saved."}), 200
        except Exception as e:
            logger.error(f"Error saving log history: {e}")
            return jsonify({"error": str(e)}), 500

    @log_bp.route("/history/fetch", methods=["GET"])
    def fetch_log_history():
        try:
            history = list(db.log_history.find().sort("timestamp", -1))
            serialized_history = [serialize_log(entry) for entry in history]
            return jsonify({
                "status": "success",
                "history": serialized_history,
                "count": len(serialized_history),
            }), 200
        except Exception as e:
            logger.error(f"Error fetching log history: {e}")
            return jsonify({"error": str(e)}), 500

================
File: routes/pbq_routes.py
================
import logging
import json
import traceback
from flask import Blueprint, request, jsonify, Response
from flask_cors import CORS

# Import your PBQ AI helper methods
from helpers.pbq_ai_helper import (
    generate_advanced_pbq,
    generate_advanced_pbq_stream,
    simulate_nmap_command_openai
)

pbq_bp = Blueprint('pbq_bp', __name__)
CORS(pbq_bp, resources={r"/api/*": {"origins": "*"}})  # Allow cross-origin for debugging or different front-end

logger = logging.getLogger(__name__)

@pbq_bp.route('/generate_pbq', methods=['POST'])
def generate_pbq():
    """
    Non-streaming route to generate a complete PBQ JSON.
    Expects JSON payload:
    {
      "category": "Network Security",
      "difficulty": "Intermediate",
      "performance_level": "average"
    }
    Returns:
      - 200: complete PBQ JSON
      - 500: error
    """
    data = request.get_json(silent=True) or {}
    category = data.get("category", "Network Security")
    difficulty = data.get("difficulty", "Intermediate")
    performance_level = data.get("performance_level", "average")

    logger.info("generate_pbq called: category=%s, difficulty=%s, performance_level=%s",
                category, difficulty, performance_level)

    try:
        pbq_json_str = generate_advanced_pbq(category, difficulty, performance_level)
        # Validate JSON structure
        pbq_data = json.loads(pbq_json_str)
        return jsonify(pbq_data), 200
    except json.JSONDecodeError as jde:
        logger.error("JSON decode error in generate_pbq: %s", jde)
        return jsonify({"error": "Invalid JSON structure generated."}), 500
    except Exception as exc:
        logger.error("Error in generate_pbq: %s\n%s", exc, traceback.format_exc())
        return jsonify({"error": str(exc)}), 500


@pbq_bp.route('/generate_pbq_stream', methods=['POST'])
def generate_pbq_stream():
    """
    Streaming route (SSE) to generate PBQ JSON chunk by chunk.
    Expects JSON payload:
    {
      "category": "Network Security",
      "difficulty": "Intermediate",
      "performance_level": "average"
    }
    Returns:
      - SSE data lines in the form "data: <partial JSON>"
    """
    data = request.get_json(silent=True) or {}
    category = data.get("category", "Network Security")
    difficulty = data.get("difficulty", "Intermediate")
    performance_level = data.get("performance_level", "average")

    logger.info("generate_pbq_stream: category=%s, difficulty=%s, performance_level=%s",
                category, difficulty, performance_level)

    def sse_stream():
        try:
            for chunk in generate_advanced_pbq_stream(category, difficulty, performance_level):
                if isinstance(chunk, str):
                    # strip out newline chars to avoid confusion
                    safe_chunk = chunk.replace('\n', '').replace('\r', '')
                    yield f"data: {safe_chunk}\n\n"
                else:
                    logger.warning("Received non-string chunk from generator: %s", chunk)
        except Exception as e:
            logger.error("Error streaming PBQ: %s\n%s", e, traceback.format_exc())
            error_msg = json.dumps({"error": f"Failed to stream PBQ: {str(e)}"})
            yield f"data: {error_msg}\n\n"

    return Response(sse_stream(), mimetype='text/event-stream')


@pbq_bp.route('/simulate_cmd', methods=['POST'])
def simulate_cmd():
    """
    Route to simulate Nmap output from a user-typed command.
    Expects JSON: {"command": "nmap -sV -p 80,443 192.168.1.10"}
    Returns: { "output": "<Simulated Nmap output text>" }
    """
    data = request.get_json(silent=True) or {}
    user_command = data.get("command", "").strip()

    logger.info("simulate_cmd called with command: %s", user_command)

    if not user_command:
        return jsonify({"error": "No command provided."}), 400

    try:
        simulated_output = simulate_nmap_command_openai(user_command)
        return jsonify({"output": simulated_output}), 200
    except Exception as exc:
        logger.error("Error in simulate_cmd: %s\n%s", exc, traceback.format_exc())
        return jsonify({"error": str(exc)}), 500

================
File: routes/scenario_routes.py
================
import logging
import json  
from flask import Blueprint, request, Response, jsonify
from helpers.scenario_helper import (
    generate_scenario,
    generate_interactive_questions,
    break_down_scenario
)

scenario_bp = Blueprint('scenario_bp', __name__)
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

@scenario_bp.route('/stream_scenario', methods=['POST'])
def stream_scenario_endpoint():
    """
    Streams scenario text in real time (token-by-token).
    Expects JSON with { industry, attack_type, skill_level, threat_intensity }
    Returns a text/plain streaming response.
    """
    data = request.get_json() or {}
    required_fields = ["industry", "attack_type", "skill_level", "threat_intensity"]
    missing = [f for f in required_fields if f not in data]
    if missing:
        logger.error(f"Missing required fields: {missing}")
        return jsonify({"error": f"Missing required fields: {missing}"}), 400

    industry = data["industry"]
    attack_type = data["attack_type"]
    skill_level = data["skill_level"]
    threat_intensity = data["threat_intensity"]

    try:
        threat_intensity = int(threat_intensity)
    except ValueError:
        logger.error("Invalid threat_intensity value; must be an integer.")
        return jsonify({"error": "threat_intensity must be an integer"}), 400

    def generate_chunks():
        scenario_generator = generate_scenario(industry, attack_type, skill_level, threat_intensity)
        for chunk in scenario_generator:
            yield chunk

    return Response(generate_chunks(), mimetype='text/plain')


@scenario_bp.route('/stream_questions', methods=['POST'])
def stream_questions_endpoint():
    """
    Streams the interactive questions (in raw JSON form) in real time, token-by-token.
    Expects JSON with { "scenario_text": "..." }
    The front end can accumulate the text and parse once done.
    """
    data = request.get_json() or {}
    scenario_text = data.get("scenario_text", "")
    if not scenario_text:
        logger.error("Missing scenario_text in the request.")
        return jsonify({"error": "Missing scenario_text"}), 400

    logger.debug(f"Received scenario_text: {scenario_text[:100]}...")  

    def generate_json_chunks():
        questions = generate_interactive_questions(scenario_text)
        if isinstance(questions, list):
            logger.debug("Questions are a list. Serializing to JSON.")
            yield json.dumps(questions)
        elif callable(questions):
            logger.debug("Questions are being streamed.")
            for chunk in questions():
                yield chunk
        else:
            logger.error("Unexpected type for questions.")
            yield json.dumps([{"error": "Failed to generate questions."}])

    return Response(generate_json_chunks(), mimetype='application/json')

================
File: routes/status_routes.py
================
from flask import Blueprint, jsonify, request
from functools import wraps
import os

from helpers.status_helper import get_system_info, get_application_status

status_bp = Blueprint('status_routes', __name__)

def require_api_key(f):
    @wraps(f)
    def decorated(*args, **kwargs):
        api_key = request.headers.get('x-api-key')
        if not api_key or api_key != os.getenv("ADMIN_API_KEY"):
            return jsonify({"error": "Unauthorized"}), 401
        return f(*args, **kwargs)
    return decorated

@status_bp.route('/status', methods=['GET'])
@require_api_key
def get_status():
    """
    GET /status/status
    Headers: { "x-api-key": "<ADMIN_API_KEY>" }
    """
    try:
        system_info = get_system_info()
        app_status = get_application_status()
        return jsonify({"system_info": system_info, "application_status": app_status}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500

================
File: routes/subscribe_routes.py
================
# routes/subscribe_routes.py

from flask import Blueprint, request, jsonify
from models.user_subscription import add_subscription, find_subscription, get_all_subscribers
from functools import wraps
import os
import re

subscribe_bp = Blueprint('subscribe_routes', __name__)

def is_valid_email(email):
    """
    Validates the email format using regex.
    """
    regex = r'^\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    return re.match(regex, email)

def require_api_key(f):
    """
    Decorator to require API key for certain routes.
    """
    @wraps(f)
    def decorated(*args, **kwargs):
        api_key = request.headers.get('x-api-key')
        if not api_key or api_key != os.getenv("ADMIN_API_KEY"):
            return jsonify({"error": "Unauthorized"}), 401
        return f(*args, **kwargs)
    return decorated

@subscribe_bp.route('/', methods=['POST'])
def subscribe():
    """
    Public route to subscribe a user.
    POST /subscribe
    Body: { "email": "user@example.com" }
    """
    try:
        data = request.get_json()
        email = data.get("email")
        if not email:
            return jsonify({"error": "Missing email"}), 400

        if not is_valid_email(email):
            return jsonify({"error": "Invalid email format."}), 400

        if find_subscription(email):
            return jsonify({"message": "You are already subscribed."}), 400

        add_subscription(email)
        return jsonify({"message": "Subscription successful!"}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

@subscribe_bp.route('/all', methods=['GET'])
@require_api_key
def get_subscribers():
    """
    Admin route to retrieve all subscribers.
    GET /subscribe/all
    Headers: { "x-api-key": "<ADMIN_API_KEY>" }
    """
    try:
        subscribers = get_all_subscribers()
        emails = [sub["email"] for sub in subscribers]
        return jsonify({"subscribers": emails}), 200
    except Exception as e:
        return jsonify({"error": str(e)}), 500

================
File: routes/test_routes.py
================
# src/routes/test_routes.py

from flask import Blueprint, request, jsonify
from bson.objectid import ObjectId
from datetime import datetime

# Mongo collections
from mongodb.database import (
    mainusers_collection,
    shop_collection,
    achievements_collection,
    tests_collection,
    testAttempts_collection,
    correctAnswers_collection
)

# Models
from models.test import (
    get_user_by_identifier,
    create_user,
    get_user_by_id,
    update_user_coins,
    update_user_xp,
    apply_daily_bonus,
    get_shop_items,
    purchase_item,
    get_achievements,
    get_test_by_id_and_category,
    check_and_unlock_achievements,
    validate_username,
    validate_email,
    validate_password,
    update_user_fields,
    get_user_by_id
)

api_bp = Blueprint('test', __name__)

def serialize_user(user):
    """Helper to convert _id, etc. to strings if needed."""
    if not user:
        return None
    user['_id'] = str(user['_id'])
    if 'currentAvatar' in user and user['currentAvatar']:
        user['currentAvatar'] = str(user['currentAvatar'])
    if 'purchasedItems' in user and isinstance(user['purchasedItems'], list):
        user['purchasedItems'] = [str(item) for item in user['purchasedItems']]
    return user

# -------------------------------------------------------------------
# USER ROUTES
# -------------------------------------------------------------------

@api_bp.route('/user/<user_id>', methods=['GET'])
def get_user(user_id):
    user = get_user_by_id(user_id)
    if not user:
        return jsonify({"error": "User not found"}), 404
    user = serialize_user(user)
    # Make sure password is included in the response, if that's desired
    if "password" not in user:
        user["password"] = user.get("password")
    return jsonify(user), 200


@api_bp.route('/user', methods=['POST'])
def register_user():
    """
    Registration: /api/user
    Expects {username, email, password, confirmPassword} in JSON
    Calls create_user, returns {message, user_id} or error.
    """
    user_data = request.json or {}
    try:
        user_id = create_user(user_data)
        return jsonify({"message": "User created", "user_id": str(user_id)}), 201
    except ValueError as ve:
        return jsonify({"error": str(ve)}), 400
    except Exception as e:
        return jsonify({"error": "Internal server error", "details": str(e)}), 500


@api_bp.route('/login', methods=['POST'])
def login():
    """
    Login: /api/login
    Expects { usernameOrEmail, password } in JSON
    If success => return user doc in JSON (serialized)
    """
    data = request.json
    if not data:
        return jsonify({"error": "No JSON data provided"}), 400

    identifier = data.get("usernameOrEmail")
    password = data.get("password")
    if not identifier or not password:
        return jsonify({"error": "Username (or Email) and password are required"}), 400

    user = get_user_by_identifier(identifier)
    if not user or user.get("password") != password:
        return jsonify({"error": "Invalid username or password"}), 401

    user = serialize_user(user)
    return jsonify({
        "user_id": user["_id"],
        "username": user["username"],
        "email": user.get("email", ""),
        "coins": user.get("coins", 0),
        "xp": user.get("xp", 0),
        "level": user.get("level", 1),
        "achievements": user.get("achievements", []),
        "xpBoost": user.get("xpBoost", 1.0),
        "currentAvatar": user.get("currentAvatar"),
        "nameColor": user.get("nameColor"),
        "purchasedItems": user.get("purchasedItems", []),
        "subscriptionActive": user.get("subscriptionActive", False),
        "password": user.get("password")
    }), 200


@api_bp.route('/user/<user_id>/daily-bonus', methods=['POST'])
def daily_bonus(user_id):
    result = apply_daily_bonus(user_id)
    if not result:
        return jsonify({"error": "User not found"}), 404
    return jsonify(result), 200


@api_bp.route('/user/<user_id>/add-xp', methods=['POST'])
def add_xp_route(user_id):
    data = request.json or {}
    xp_to_add = data.get("xp", 0)
    updated = update_user_xp(user_id, xp_to_add)
    if not updated:
        return jsonify({"error": "User not found"}), 404
    new_achievements = check_and_unlock_achievements(user_id)
    updated["newAchievements"] = new_achievements
    return jsonify(updated), 200


@api_bp.route('/user/<user_id>/add-coins', methods=['POST'])
def add_coins_route(user_id):
    data = request.json or {}
    coins_to_add = data.get("coins", 0)
    update_user_coins(user_id, coins_to_add)
    return jsonify({"message": "Coins updated"}), 200


# -------------------------------------------------------------------
# SHOP ROUTES
# -------------------------------------------------------------------

@api_bp.route('/shop', methods=['GET'])
def fetch_shop():
    items = get_shop_items()
    for item in items:
        item["_id"] = str(item["_id"])
    return jsonify(items), 200


@api_bp.route('/shop/purchase/<item_id>', methods=['POST'])
def purchase_item_route(item_id):
    data = request.json or {}
    user_id = data.get("userId")
    if not user_id:
        return jsonify({"success": False, "message": "userId is required"}), 400

    result = purchase_item(user_id, item_id)
    if result["success"]:
        return jsonify(result), 200
    else:
        return jsonify(result), 400


@api_bp.route('/shop/equip', methods=['POST'])
def equip_item_route():
    data = request.json or {}
    user_id = data.get("userId")
    item_id = data.get("itemId")

    if not user_id or not item_id:
        return jsonify({"success": False, "message": "userId and itemId are required"}), 400

    user = get_user_by_id(user_id)
    if not user:
        return jsonify({"success": False, "message": "User not found"}), 404

    try:
        oid = ObjectId(item_id)
    except Exception:
        return jsonify({"success": False, "message": "Invalid item ID"}), 400

    item_doc = shop_collection.find_one({"_id": oid})
    if not item_doc:
        return jsonify({"success": False, "message": "Item not found in shop"}), 404

    # If user hasn't purchased it, check level-based unlock
    if oid not in user.get("purchasedItems", []):
        if user.get("level", 1) < item_doc.get("unlockLevel", 1):
            return jsonify({"success": False, "message": "Item not unlocked"}), 400

    # Equip the avatar
    mainusers_collection.update_one(
        {"_id": user["_id"]},
        {"$set": {"currentAvatar": oid}}
    )
    return jsonify({"success": True, "message": "Avatar equipped"}), 200


# -------------------------------------------------------------------
# TESTS ROUTES
# -------------------------------------------------------------------

@api_bp.route('/tests/<test_id>', methods=['GET'])
def fetch_test_by_id_route(test_id):
    # This is your original single-parameter route
    test_doc = get_test_by_id_and_category(test_id, None)  # or your old get_test_by_id
    if not test_doc:
        return jsonify({"error": "Test not found"}), 404
    test_doc["_id"] = str(test_doc["_id"])
    return jsonify(test_doc), 200


@api_bp.route('/tests/<category>/<test_id>', methods=['GET'])
def fetch_test_by_category_and_id(category, test_id):
    """
    NEW route that fetches a test doc by both category and testId
    e.g. /tests/aplus/1
    """
    try:
        test_id_int = int(test_id)
    except Exception:
        return jsonify({"error": "Invalid test ID"}), 400

    test_doc = tests_collection.find_one({
        "testId": test_id_int,
        "category": category
    })
    if not test_doc:
        return jsonify({"error": "Test not found"}), 404

    test_doc["_id"] = str(test_doc["_id"])
    return jsonify(test_doc), 200


# -------------------------------------------------------------------
# PROGRESS / ATTEMPTS ROUTES
# -------------------------------------------------------------------

@api_bp.route('/attempts/<user_id>/<test_id>', methods=['GET'])
def get_test_attempt(user_id, test_id):
    """
    Returns either an unfinished attempt if it exists;
    otherwise returns the most recently finished attempt for that user/test.
    This version searches for testId as either an integer or a string.
    """
    try:
        user_oid = ObjectId(user_id)
        try:
            test_id_int = int(test_id)
        except:
            test_id_int = None
    except:
        return jsonify({"error": "Invalid user ID or test ID"}), 400

    # Build query with $or for testId
    query = {"userId": user_oid, "finished": False}
    if test_id_int is not None:
        query["$or"] = [{"testId": test_id_int}, {"testId": test_id}]
    else:
        query["testId"] = test_id

    attempt = testAttempts_collection.find_one(query)

    # If no unfinished attempt, check the most recent finished one
    if not attempt:
        query_finished = {"userId": user_oid, "finished": True}
        if test_id_int is not None:
            query_finished["$or"] = [{"testId": test_id_int}, {"testId": test_id}]
        else:
            query_finished["testId"] = test_id
        attempt = testAttempts_collection.find_one(query_finished, sort=[("finishedAt", -1)])

    if not attempt:
        return jsonify({"attempt": None}), 200

    attempt["_id"] = str(attempt["_id"])
    attempt["userId"] = str(attempt["userId"])
    return jsonify({"attempt": attempt}), 200


@api_bp.route('/attempts/<user_id>/<test_id>', methods=['POST'])
def update_test_attempt(user_id, test_id):
    data = request.json or {}
    try:
        user_oid = ObjectId(user_id)
        try:
            test_id_int = int(test_id)
        except:
            test_id_int = test_id
    except:
        return jsonify({"error": "Invalid user ID or test ID"}), 400

    filter_ = {"userId": user_oid, "finished": False, "$or": [{"testId": test_id_int}, {"testId": test_id}]}
    update_doc = {
        "$set": {
            "userId": user_oid,
            "testId": test_id_int if isinstance(test_id_int, int) else test_id,
            "category": data.get("category", "global"),
            "answers": data.get("answers", []),
            "score": data.get("score", 0),
            "totalQuestions": data.get("totalQuestions", 0),
            "currentQuestionIndex": data.get("currentQuestionIndex", 0),
            "shuffleOrder": data.get("shuffleOrder", []),
            "finished": data.get("finished", False)
        }
    }
    testAttempts_collection.update_one(filter_, update_doc, upsert=True)
    return jsonify({"message": "Progress updated"}), 200


@api_bp.route('/attempts/<user_id>/<test_id>/finish', methods=['POST'])
def finish_test_attempt(user_id, test_id):
    data = request.json or {}
    try:
        user_oid = ObjectId(user_id)
        try:
            test_id_int = int(test_id)
        except:
            test_id_int = test_id
    except:
        return jsonify({"error": "Invalid user ID or test ID"}), 400

    filter_ = {"userId": user_oid, "finished": False, "$or": [{"testId": test_id_int}, {"testId": test_id}]}
    update_doc = {
        "$set": {
            "finished": True,
            "finishedAt": datetime.utcnow(),
            "score": data.get("score", 0),
            "totalQuestions": data.get("totalQuestions", 0),
        }
    }
    testAttempts_collection.update_one(filter_, update_doc)

    newly_unlocked = check_and_unlock_achievements(user_id)
    return jsonify({
        "message": "Test attempt finished",
        "newlyUnlocked": newly_unlocked
    }), 200


@api_bp.route('/attempts/<user_id>/list', methods=['GET'])
def list_test_attempts(user_id):
    try:
        user_oid = ObjectId(user_id)
    except:
        return jsonify({"error": "Invalid user ID"}), 400

    page = request.args.get("page", default=1, type=int)
    page_size = request.args.get("page_size", default=50, type=int)
    skip_count = (page - 1) * page_size

    cursor = testAttempts_collection.find(
        {"userId": user_oid}
    ).sort("finishedAt", -1).skip(skip_count).limit(page_size)

    attempts = []
    for doc in cursor:
        doc["_id"] = str(doc["_id"])
        doc["userId"] = str(doc["userId"])
        attempts.append(doc)

    return jsonify({
        "page": page,
        "page_size": page_size,
        "attempts": attempts
    }), 200


# -------------------------------------------------------------------
# FIRST-TIME-CORRECT ANSWERS
# -------------------------------------------------------------------
@api_bp.route('/user/<user_id>/submit-answer', methods=['POST'])
def submit_answer(user_id):
    data = request.json or {}
    test_id = str(data.get("testId"))
    question_id = data.get("questionId")
    selected_index = data.get("selectedIndex")
    correct_index = data.get("correctAnswerIndex")
    xp_per_correct = data.get("xpPerCorrect", 10)
    coins_per_correct = data.get("coinsPerCorrect", 5)

    user = get_user_by_id(user_id)
    if not user:
        return jsonify({"error": "User not found"}), 404

    is_correct = (selected_index == correct_index)
    already_correct = correctAnswers_collection.find_one({
        "userId": user["_id"],
        "testId": test_id,
        "questionId": question_id
    })

    awarded_xp = 0
    awarded_coins = 0
    if is_correct and not already_correct:
        correctAnswers_collection.insert_one({
            "userId": user["_id"],
            "testId": test_id,
            "questionId": question_id
        })
        update_user_xp(user_id, xp_per_correct)
        update_user_coins(user_id, coins_per_correct)
        awarded_xp = xp_per_correct
        awarded_coins = coins_per_correct

    updated_user = get_user_by_id(user_id)
    new_xp = updated_user.get("xp", 0)
    new_coins = updated_user.get("coins", 0)

    return jsonify({
        "isCorrect": is_correct,
        "alreadyCorrect": True if already_correct else False,
        "awardedXP": awarded_xp,
        "awardedCoins": awarded_coins,
        "newXP": new_xp,
        "newCoins": new_coins
    }), 200


# -------------------------------------------------------------------
# ACHIEVEMENTS
# -------------------------------------------------------------------
@api_bp.route('/achievements', methods=['GET'])
def fetch_achievements_route():
    ach_list = get_achievements()
    for ach in ach_list:
        ach["_id"] = str(ach["_id"])
    return jsonify(ach_list), 200


# -------------------------------------------------------------------
# Leaderboard Route
# -------------------------------------------------------------------
@api_bp.route('/leaderboard', methods=['GET'])
def get_leaderboard():
    top_users_cursor = mainusers_collection.find(
        {},
        {"username": 1, "level": 1, "xp": 1, "currentAvatar": 1}
    ).sort("level", -1).limit(100)

    results = []
    rank = 1
    for user in top_users_cursor:
        user_data = {
            "username": user.get("username", "unknown"),
            "level": user.get("level", 1),
            "xp": user.get("xp", 0),
            "rank": rank,
            "avatarUrl": None
        }
        if user.get("currentAvatar"):
            avatar_item = shop_collection.find_one({"_id": user["currentAvatar"]})
            if avatar_item and "imageUrl" in avatar_item:
                user_data["avatarUrl"] = avatar_item["imageUrl"]

        results.append(user_data)
        rank += 1

    return jsonify(results), 200


# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# USERNAME/EMAIL/PASSWORD CHANGES
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
@api_bp.route('/user/change-username', methods=['POST'])
def change_username():
    data = request.json or {}
    user_id = data.get("userId")
    new_username = data.get("newUsername")
    if not user_id or not new_username:
        return jsonify({"error": "Missing userId or newUsername"}), 400

    # Validate new username using the new rules.
    valid, errors = validate_username(new_username)
    if not valid:
        return jsonify({"error": "Invalid new username", "details": errors}), 400

    # Check if username is already taken.
    if mainusers_collection.find_one({"username": new_username}):
        return jsonify({"error": "Username already taken"}), 400

    doc = get_user_by_id(user_id)
    if not doc:
        return jsonify({"error": "User not found"}), 404

    update_user_fields(user_id, {"username": new_username})
    return jsonify({"message": "Username updated"}), 200


@api_bp.route('/user/change-email', methods=['POST'])
def change_email():
    data = request.json or {}
    user_id = data.get("userId")
    new_email = data.get("newEmail")
    if not user_id or not new_email:
        return jsonify({"error": "Missing userId or newEmail"}), 400

    # Validate new email using the new rules.
    valid, errors = validate_email(new_email)
    if not valid:
        return jsonify({"error": "Invalid email", "details": errors}), 400

    if mainusers_collection.find_one({"email": new_email}):
        return jsonify({"error": "Email already in use"}), 400

    doc = get_user_by_id(user_id)
    if not doc:
        return jsonify({"error": "User not found"}), 404

    update_user_fields(user_id, {"email": new_email})
    return jsonify({"message": "Email updated"}), 200


@api_bp.route('/user/change-password', methods=['POST'])
def change_password():
    data = request.json or {}
    user_id = data.get("userId")
    old_password = data.get("oldPassword")
    new_password = data.get("newPassword")
    confirm = data.get("confirmPassword")

    if not user_id or not old_password or not new_password or not confirm:
        return jsonify({"error": "All fields are required"}), 400
    if new_password != confirm:
        return jsonify({"error": "New passwords do not match"}), 400

    # Validate the new password using the new rules.
    valid, errors = validate_password(new_password)
    if not valid:
        return jsonify({"error": "Invalid new password", "details": errors}), 400

    user_doc = get_user_by_id(user_id)
    if not user_doc:
        return jsonify({"error": "User not found"}), 404

    # NOTE: This example compares plain-text passwords.
    # In production, ensure you hash passwords and use a proper verification method.
    if user_doc.get("password") != old_password:
        return jsonify({"error": "Old password is incorrect"}), 401

    update_user_fields(user_id, {"password": new_password})
    return jsonify({"message": "Password updated"}), 200


@api_bp.route('/subscription/cancel', methods=['POST'])
def cancel_subscription():
    """
    Placeholder. Possibly set subscriptionActive=False
    """
    return jsonify({"message": "Cancel subscription placeholder"}), 200

================
File: routes/unsubscribe_routes.py
================
# routes/unsubscribe_routes.py

from flask import Blueprint, request, jsonify
from models.user_subscription import remove_subscription, find_subscription
import re

unsubscribe_bp = Blueprint('unsubscribe_routes', __name__)

def is_valid_email(email):
    regex = r'^\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    return re.match(regex, email)

@unsubscribe_bp.route('/', methods=['POST'])
def unsubscribe():
    """
    POST /unsubscribe
    Body: { "email": "user@example.com" }
    """
    try:
        data = request.get_json()
        email = data.get("email")
        if not email:
            return jsonify({"error": "Missing email"}), 400

        if not find_subscription(email):
            return jsonify({"error": "You are not subscribed."}), 404

        remove_subscription(email)
        return jsonify({"message": "Successfully unsubscribed"}), 200

    except Exception as e:
        return jsonify({"error": str(e)}), 500

================
File: routes/xploit_routes.py
================
from flask import Blueprint, request, jsonify, Response
from helpers.xploitcraft_helper import Xploits
import logging

logger = logging.getLogger(__name__)

xploit = Xploits()
xploit_bp = Blueprint('xploit_bp', __name__)

@xploit_bp.route('/generate_payload', methods=['POST'])
def generate_payload_endpoint():
    data = request.get_json()
    logger.debug(f"Received data: {data}")

    if not data or (not data.get('vulnerability') and not data.get('evasion_technique')):
        logger.error("Invalid request payload - need at least one of vulnerability or evasion_technique")
        return jsonify({'error': 'Please provide at least one of vulnerability or evasion_technique'}), 400

    vulnerability = data.get('vulnerability', "")
    evasion_technique = data.get('evasion_technique', "")
    stream_requested = data.get('stream', False)

    try:
        if stream_requested:
            def generate():
                for chunk in xploit.generate_exploit_payload(vulnerability, evasion_technique, stream=True):
                    yield chunk

            return Response(generate(), mimetype='text/plain')
        else:
            payload = xploit.generate_exploit_payload(vulnerability, evasion_technique, stream=False)
            logger.debug(f"Generated payload: {payload}")
            return jsonify({'payload': payload})

    except Exception as e:
        logger.error(f"Error while generating payload: {str(e)}")
        return jsonify({'error': 'Failed to generate payload'}), 500

================
File: app.py
================
#ProxyAuthRequired/backend/app.py

from flask import Flask
from flask_socketio import SocketIO
from dotenv import load_dotenv
from flask_cors import CORS
from flask_session import Session
from pymongo import MongoClient
import redis
import os
import logging
from flask import request, jsonify

# Import your existing routes
from routes.xploit_routes import xploit_bp
from routes.scenario_routes import scenario_bp
from routes.analogy_routes import analogy_bp
from routes.subscribe_routes import subscribe_bp
from routes.unsubscribe_routes import unsubscribe_bp
from routes.admin_newsletter_routes import admin_newsletter_bp
from routes.grc_routes import grc_bp
from routes.log_routes import log_bp
from routes.celery_routes import celery_bp
from routes.status_routes import status_bp
from routes.pbq_routes import pbq_bp
from routes.test_routes import api_bp

# IMPORTANT: Now import from models.py (not models.user_subscription)
from models.test import create_user, get_user_by_id, update_user_fields

from mongodb.database import db

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
mongo_uri = os.getenv("MONGO_URI")

client = MongoClient(mongo_uri)
db = client.get_database()

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
CORS(app)
socketio = SocketIO(app, cors_allowed_origins="*")

app.config['SECRET_KEY'] = os.getenv('SECRET_KEY')

app.config['SESSION_TYPE'] = 'redis'
app.config['SESSION_PERMANENT'] = False
app.config['SESSION_USE_SIGNER'] = True
app.config['SESSION_KEY_PREFIX'] = 'flask_session:'
app.config['SESSION_REDIS'] = redis.StrictRedis(host='redis', port=6379, db=0)

REDIS_PASSWORD = os.getenv('REDIS_PASSWORD')
app.config['SESSION_REDIS'] = redis.StrictRedis(
    host='redis',
    port=6379,
    db=0,
    password=REDIS_PASSWORD
)

Session(app)

@app.route('/health')
def home():
    return 'Backend is running'

@app.before_request
def log_request_info():
    logger.info(f"Handling request to {request.path} with method {request.method}")

# Register all your blueprints
app.register_blueprint(xploit_bp, url_prefix='/payload')
app.register_blueprint(scenario_bp, url_prefix='/scenario')
app.register_blueprint(analogy_bp, url_prefix='/analogy')
app.register_blueprint(grc_bp, url_prefix='/grc')
app.register_blueprint(log_bp, url_prefix='/logs')
app.register_blueprint(subscribe_bp, url_prefix='/subscribe')
app.register_blueprint(unsubscribe_bp, url_prefix='/unsubscribe')
app.register_blueprint(admin_newsletter_bp, url_prefix='/admin/newsletter')
app.register_blueprint(celery_bp, url_prefix='/celery')
app.register_blueprint(status_bp, url_prefix='/status')
app.register_blueprint(pbq_bp, url_prefix='/pbq')
app.register_blueprint(api_bp, url_prefix='/test')

# ENV VAR for admin password
ADMIN_API_KEY = os.getenv("ADMIN_API_KEY")





@socketio.on('connect')
def handle_connect():
    logger.info('Client connected')
    socketio.emit('message', {'data': 'Connected to server'})

if __name__ == '__main__':
    socketio.run(app, host='0.0.0.0', port=5000, debug=True, allow_unsafe_werkzeug=True)

================
File: Dockerfile.backend
================
FROM python:3.11.3


WORKDIR /app


RUN apt-get update && apt-get install -y --no-install-recommends \
    apt-transport-https \
    ca-certificates \
    build-essential \
    libffi-dev \
    gcc \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*



RUN python3 -m venv /venv


RUN /venv/bin/pip install --upgrade pip setuptools wheel


COPY requirements.txt /app/requirements.txt


RUN /venv/bin/pip install --no-cache-dir -r /app/requirements.txt


COPY . /app


ENV VIRTUAL_ENV="/venv"
ENV PATH="/venv/bin:$PATH"
ENV FLASK_ENV=production
ENV FLASK_APP=app.py


RUN chmod +x /app/app.py


EXPOSE 5000

CMD ["/venv/bin/gunicorn", "-k", "gevent", "-b", "0.0.0.0:5000", "app:app", "--timeout", "120"]

================
File: requirements.txt
================
annotated-types==0.7.0
anyio==4.6.2.post1
bidict==0.23.1
blinker==1.9.0
certifi==2024.8.30
click==8.1.7
dnspython==2.7.0
Flask==3.0.3
Flask-SocketIO==5.4.1
httpx==0.27.2
idna==3.10
itsdangerous==2.2.0
Jinja2==3.1.4
MarkupSafe==3.0.2
openai==1.54.3
pydantic==2.9.2
python-engineio==4.10.1
python-socketio==5.11.4
simple-websocket==1.1.0
tqdm==4.67.0
typing_extensions==4.12.2
Werkzeug==3.1.3
python-dotenv==1.0.0
requests==2.31.0
gunicorn==21.2.0
Flask-CORS==3.0.10
Flask-Session
redis==5.0.0
celery==5.3.4
sendgrid==6.9.7
gevent==23.9.1
cffi==1.15.1
greenlet
faker
tzdata
Flask-Migrate
psutil
pymongo[srv]
Flask_Pymongo

================
File: update_newsletter.py
================
import os
from pymongo import MongoClient
from helpers.daily_newsletter_helper import set_current_newsletter_db
from dotenv import load_dotenv

# Load environment variables from .env if needed
load_dotenv()

def main():
    # Define your new newsletter content
    new_content = """
<html>
<body>
<p>Welcome to today's edition of our Cybersecurity Newsletter! I hope you find these insights valuable and informative.</p> 

<!-- Exam Objective Tip of the Day -->
<h2>Exam Objective Tip of the Day: Understanding DNS Records</h2>
<p><strong>MX, TXT, A, AAAA, CNAME, and More: DNS Records Explained Through a Postal Scenario</strong></p>
<p>Imagine DNS records as different components of a postal system that ensure your letters (emails and data) reach the correct destination. Here's how MX, TXT, A, AAAA, CNAME, and other DNS records function within this system:</p>
<ul>
    <li><strong>A Record:</strong> Think of the A record as the street address of a building. It maps a domain name (like example.com) to its corresponding IPv4 address (e.g., 192.0.2.1), allowing users to locate the website.</li>
    <li><strong>AAAA Record:</strong> Similar to the A record, the AAAA record provides a street address but for IPv6 addresses (e.g., 2001:0db8:85a3:0000:0000:8a2e:0370:7334), supporting the newer internet protocol.</li>
    <li><strong>MX Record:</strong> The MX record is like the post office responsible for handling incoming mail for a domain. It directs emails to the correct mail servers by specifying their addresses and priority.</li>
    <li><strong>TXT Record:</strong> TXT records are akin to special instructions or notices attached to mail. They can contain information like SPF (Sender Policy Framework) records, which help verify the authenticity of incoming emails, reducing spam and phishing attempts.</li>
    <li><strong>CNAME Record:</strong> The CNAME record acts like an alias or nickname. It allows multiple domain names to point to the same IP address or canonical domain name, simplifying DNS management and ensuring consistency across services.</li>
    <li><strong>NS Record:</strong> NS records are like the main office branches responsible for managing all postal operations within a region. They specify the authoritative name servers for a domain, directing all DNS queries to the right servers.</li>
    <li><strong>PTR Record:</strong> PTR records function as reverse DNS lookups, translating IP addresses back to domain names. They are akin to verifying the return address on a letter to ensure authenticity.</li>
</ul>
<p><strong>Practical Application:</strong> By properly configuring these DNS records, your domain ensures that websites are reachable, emails are correctly routed, and email authenticity is maintained, enhancing both accessibility and security.</p>

<p><strong>Real-Life Scenario:</strong> Consider a global company like "GlobalMail" that operates in multiple countries and handles a large volume of emails daily. Here's how each DNS record is utilized:</p>
<ul>
    <li><strong>A Record:</strong> GlobalMail's website (globalmail.com) has an A record pointing to its primary server's IPv4 address, ensuring that users can access the website reliably.</li>
    <li><strong>AAAA Record:</strong> As GlobalMail transitions to IPv6, its AAAA record points to the server's IPv6 address, providing better scalability and performance for users on the newer protocol.</li>
    <li><strong>MX Record:</strong> GlobalMail's MX records specify multiple mail servers with different priorities. This setup ensures that if the primary mail server is down, emails are automatically routed to a backup server without disruption.</li>
    <li><strong>TXT Record:</strong> GlobalMail uses TXT records to publish SPF and DKIM (DomainKeys Identified Mail) information. This configuration helps recipient mail servers verify that emails from globalmail.com are legitimate, reducing the chances of emails being marked as spam.</li>
    <li><strong>CNAME Record:</strong> GlobalMail uses CNAME records to create aliases for various services. For example, mail.globalmail.com is a CNAME for mailserver.globalmail.com, simplifying DNS management and ensuring consistency across their email infrastructure.</li>
    <li><strong>NS Record:</strong> GlobalMail's NS records point to their primary and secondary name servers, ensuring that DNS queries are handled efficiently and reliably across different regions.</li>
    <li><strong>PTR Record:</strong> For security and verification, GlobalMail sets up PTR records for their mail servers, ensuring that IP addresses resolve back to the correct domain names, thereby enhancing trustworthiness and reducing the risk of phishing attacks.</li>
</ul>
<p><strong>Conjunction of Records:</strong> Together, these DNS records create a robust and secure email and website infrastructure. The A and AAAA records ensure that the website is accessible via both IPv4 and IPv6, while the MX records guarantee reliable email delivery. The TXT records provide necessary security measures to authenticate emails, preventing malicious actors from spoofing the domain. The CNAME records simplify DNS management by allowing multiple domain names to point to the same services, ensuring consistency and ease of maintenance. NS records ensure that DNS queries are directed to authoritative servers, and PTR records add an extra layer of verification and security.</p>

<!-- Pen-Testing Tool Tip and Trick of the Day: WinRM -->
<h2>Pen-Testing Tool Tip and Trick of the Day: WinRM</h2>
<p>WinRM (Windows Remote Management) is a powerful tool for remotely managing Windows systems. It's essential for administrators but can be exploited by attackers if not properly secured.</p>
<p><strong>Useful Command:</strong></p>
<pre><code>winrm set winrm/config/client '@{TrustedHosts="*" }'</code></pre>
<p><strong>Command Breakdown:</strong></p>
<ul>
    <li><strong>winrm set:</strong> Initiates a configuration change for WinRM.</li>
    <li><strong>winrm/config/client:</strong> Specifies that the configuration change applies to the WinRM client settings.</li>
    <li><strong>'@{TrustedHosts="*" }':</strong> Sets the TrustedHosts list to accept connections from any host. The asterisk (*) is a wildcard that allows all hosts, which can be risky if not managed properly.</li>
</ul>
<p><strong>Security Tip:</strong> While setting TrustedHosts to "*" is useful for testing, it poses significant security risks. Always specify trusted hosts explicitly to minimize exposure to unauthorized access.</p>

<!-- Study Tip of the Day -->
<h2>Study Tip of the Day: Design Your Workspace to Minimize Visual Clutter</h2>
<p>Creating an organized and uncluttered workspace is pivotal for maintaining focus and enhancing productivity. A well-designed environment reduces distractions, allowing you to concentrate better on the task at hand.</p>
<p><strong>Why It Matters:</strong> Visual clutter can overwhelm your brain, leading to decreased attention and increased stress. By minimizing unnecessary items, you streamline your focus, making it easier to prioritize tasks and manage time effectively.</p>
<p><strong>How to Apply:</strong> Start by decluttering your desk. Remove items that are not essential for your current projects. Utilize organizers, drawers, and shelves to keep necessary tools within reach but out of sight. Regularly assess your workspace to maintain order and adapt it to your evolving needs.</p>
<p><strong>Practical Example:</strong> Imagine you're preparing for a critical cybersecurity exam. By organizing your study materials, such as notes, flashcards, and reference books, in designated areas, you can quickly access what you need without getting distracted by unrelated items. This setup not only saves time but also creates a conducive environment for effective learning.</p>

<!-- News Summarization -->
<h2>News Summarization: AWS Repeats RCE Vulnerability in Neuron SDK</h2>
<p>Amazon Web Services (AWS) has introduced the same remote code execution (RCE) vulnerability three times over the last four years through its Neuron SDK, highlighting critical lapses in securing its Python package installation processes.</p>
<p>Despite previous warnings and fixes, the same dependency confusion vulnerability has resurfaced with new package releases in its software ecosystem.</p>
<p>The issue was first discovered in April 2022 when Giraffe Security flagged a vulnerability in AWS’s Neuron SDK, a set of Python libraries enabling machine learning workloads on AWS’s specialized hardware.</p>
<p>The problem stemmed from AWS’s official installation instructions and documentation, which recommended a command like the following:</p>
<pre><code>pip install transformers-neuronx --extra-index-url=https://pip.repos.neuron.amazonaws.com</code></pre>
<p>At a glance, the command seems simple, instructing Python’s pip package manager to install the package transformers-neuronx from the AWS-specific repository (https://pip.repos.neuron.amazonaws.com). However, this approach contains a hidden danger rooted in how pip handles the parameters.</p>
<p><strong>The Technical Issue:</strong> The --extra-index-url parameter does not exclusively restrict package downloads to the specified private repository.</p>
<p>Instead, it allows pip to search the default public PyPi repository for packages, falling back on it if the package is not found in the specified index. This creates a critical vulnerability: malicious actors could upload a package with the same name to PyPi, tricking users into downloading and executing malicious code.</p>
<p>In 2022, Giraffe Security confirmed this vulnerability by claiming unprotected AWS package names like mx-neuron on PyPi and reporting the flaw through AWS’s bug bounty program.</p>
<p>AWS promptly addressed the issue by uploading placeholder “dummy” versions of the affected packages to PyPi, preventing further exploitation. However, the root cause—a flawed reliance on the --extra-index-url parameter—remained unaddressed.</p>
<p>Despite being aware of the issue since at least 2020, AWS failed to implement a lasting solution, leading to repeated vulnerabilities being exposed in 2022.</p>
<p>In December 2024, Giraffe Security’s latest investigation revealed that AWS had once again introduced the same vulnerability.</p>
<p>AWS’s repeated missteps raise questions about their approach to addressing this issue. On one hand, their quick response to past reports suggests that they take the vulnerability seriously. However, the recurrence of the same flaw indicates a lack of systemic processes to prevent it.</p>
<p>This situation underscores a critical security lesson: even trusted sources like official AWS documentation are not immune to mistakes.</p>
<p>Developers should always scrutinize and fully understand package installation processes before implementing them in production systems. Safer alternatives—such as using the --index-url parameter to restrict downloads exclusively to private repositories or leveraging modern package managers like Poetry—should be considered.</p>
<p>While this recurring issue may seem like a niche vulnerability, it has broader implications for security in the cloud ecosystem.</p>
<p>Dependency confusion attacks have become a growing concern, particularly as more organizations rely on private package registries in tandem with public repositories like PyPi or npm.</p>
<p>The responsibility to mitigate these risks lies not only with end-users but also with service providers like AWS, who must ensure their tools and documentation follow security best practices.</p>

<!-- Life Tip of the Day -->
<h2>Life Tip of the Day: The Hidden Impact of Constant Negative Thinking</h2>
<p><strong>Understanding Negative Thought Patterns</strong></p>
<p>Constant negative thinking can subtly rewire your brain, making you perceive your life as worse than it truly is. This cognitive distortion not only affects your mental health but also diminishes your overall life satisfaction.</p>
<p><strong>The Psychological Mechanism:</strong> When you frequently dwell on negative thoughts, your brain starts to prioritize these patterns, reinforcing them through neural pathways. This makes it harder to recognize and appreciate positive experiences, creating a biased perspective that your life is predominantly negative.</p>
<p><strong>Practical Insight:</strong> Instead of trying to suppress negative thoughts, acknowledge them without judgment. Engage in cognitive restructuring by challenging these thoughts and reframing them into more balanced perspectives.</p>
<p><strong>How to Apply:</strong> If you catch yourself thinking, "I always fail at everything," pause and analyze the evidence. Recall instances where you've succeeded and recognize that failure is a part of growth. This shift helps prevent your mind from settling into a perpetual state of negativity.</p>
<p><strong>Real-Life Example:</strong> Imagine you're working on a challenging project and encounter a setback. Instead of thinking, "This project will never succeed," reframe it to, "This setback is an opportunity to learn and improve. I've overcome challenges before, and I can do it again."</p>

<p>Thank you for reading today's newsletter! Stay informed and stay secure.</p>
</body>
</html>
    """
    try:
        # Update the newsletter in the database
        result = set_current_newsletter_db(new_content)
        if result and (result.modified_count > 0 or result.upserted_id is not None):
            print("Newsletter updated successfully.")
        else:
            print("No changes were made to the newsletter.")
    except Exception as e:
        print(f"An error occurred while updating the newsletter: {e}")

if __name__ == "__main__":
    main()



================================================================
End of Codebase
================================================================
